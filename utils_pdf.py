import fitz  # PyMuPDF
import re
import pikepdf
import io
import os
import fitz  
import difflib



def compute_page_offset(doc, search_range=30):
    """
    è®¡ç®— PDF ç‰©ç†é¡µç ä¸é€»è¾‘é¡µç çš„åç§»é‡ã€‚
    ç­–ç•¥ï¼šæ‰«æå‰ N é¡µï¼Œå¯»æ‰¾é¡µé¢åº•éƒ¨æ ‡æœ‰ "- 1 -" æˆ– "1" çš„é¡µé¢ã€‚
    è¯¥é¡µé¢çš„ç‰©ç†ç´¢å¼• (index) å³ä¸ºåç§»é‡ offsetã€‚
    ä¾‹å¦‚ï¼šç¬¬ 5 é¡µå°ç€ "1"ï¼Œè¯´æ˜ offset = 4 (å› ä¸º index æ˜¯ 4)ã€‚
    """
    offset = 0
    try:
        # åŒ¹é…é¡µé¢åº•éƒ¨å¸¸è§çš„é¡µç æ ¼å¼ï¼š "1", "- 1 -", "Page 1"
        # æ³¨æ„ï¼šå¾ˆå¤šæ–‡æ¡£ç¬¬ä¸€é¡µæ­£æ–‡ä¸æ ‡é¡µç ï¼Œæ‰€ä»¥æˆ‘ä»¬æ‰¾ "1" æˆ–è€… "2" å€’æ¨
        page_num_patterns = [
            r"^\s*[-â€”]?\s*1\s*[-â€”]?\s*$",  # - 1 -
            r"^\s*1\s*$",                  # 1
            r"ç¬¬\s*1\s*é¡µ"                 # ç¬¬ 1 é¡µ
        ]
        
        # æ‰«æå‰ search_range é¡µ
        for i in range(min(search_range, doc.page_count)):
            page = doc[i]
            # è·å–é¡µé¢æ–‡æœ¬ï¼Œé™åˆ¶åªçœ‹åº•éƒ¨ 10% çš„åŒºåŸŸï¼ˆé¡µè„šé€šå¸¸åœ¨è¿™é‡Œï¼‰
            rect = page.rect
            footer_rect = fitz.Rect(0, rect.height * 0.9, rect.width, rect.height)
            text = page.get_text("text", clip=footer_rect).strip()
            
            # æ£€æŸ¥æ˜¯å¦åŒ¹é…â€œ1â€
            for pat in page_num_patterns:
                if re.search(pat, text):
                    print(f"ğŸ” åœ¨ç‰©ç†ç¬¬ {i+1} é¡µåº•ç«¯å‘ç°é¡µç  '1'ï¼Œè®¡ç®—åç§»é‡ offset = {i}")
                    return i
        
        # å¤‡é€‰ç­–ç•¥ï¼šå¦‚æœæ‰¾ä¸åˆ° "1"ï¼Œå°è¯•æ‰¾ "2" æˆ–è€…æ˜¯ "ç›®å½•" ç»“æŸåçš„ä¸‹ä¸€é¡µ
        # è¿™é‡Œä¸ºäº†ç¨³å¥ï¼Œå¦‚æœæ‰¾ä¸åˆ°ï¼Œå°è¯•é€šè¿‡ç›®å½•çš„ç¬¬ä¸€æ¡ç›®å€’æ¨
        # ä½†ç›®å‰ä¿æŒ 0 æ˜¯æœ€å®‰å…¨çš„é»˜è®¤å€¼ï¼ˆå³å‡è®¾å°é¢å°±æ˜¯ç¬¬1é¡µï¼‰
        print("âš ï¸ æœªèƒ½åœ¨é¡µè„šè‡ªåŠ¨æ£€æµ‹åˆ°èµ·å§‹é¡µç  '1'ï¼Œé»˜è®¤ offset = 0")
        return 0

    except Exception as e:
        print(f"è®¡ç®—åç§»é‡å‡ºé”™: {e}")
        return 0


# ========================================================
# è§£æç›®å½•ç”Ÿæˆå­—å…¸
# ========================================================
def parse_toc_to_dict(doc, max_scan_pages=20):
    """
    è§£æPDFç›®å½•ï¼Œè¿”å›ç»“æ„åŒ–å­—å…¸ï¼š
    {
        "clean_title_string": [start_page, end_page],
        ...
    }
    """
    toc_list = [] # ä¸´æ—¶å­˜å‚¨ [(title, page), ...]
    full_toc_text = ""

    # --- A. æå–å‰Né¡µæ–‡æœ¬ ---
    for i in range(min(max_scan_pages, doc.page_count)):
        try:
            page_text = doc[i].get_text()
            if page_text:
                full_toc_text += page_text + "\n"
        except:
            continue

    # --- B. æ¸…æ´—æ–‡æœ¬ (å…³é”®æ­¥éª¤) ---
    # 1. å»é™¤ç›®å½•ä¸­çš„è™šçº¿/ç‚¹ (å¦‚ "......")
    clean_text = re.sub(r"[â€¦\.ï¼]{2,}", " ", full_toc_text)
    # 2. å°è¯•ä¿®å¤æ¢è¡Œ (æœ‰äº›æ ‡é¢˜è¢«æ–­æˆä¸¤è¡Œï¼Œé€šå¸¸ä¸‹ä¸€è¡Œæ˜¯é¡µç )
    # è¿™ä¸€æ­¥æ¯”è¾ƒæ¿€è¿›ï¼Œæ ¹æ®å®é™…æƒ…å†µå¾®è°ƒ
    # clean_text = re.sub(r'\n\s*(\d+)', r' \1', clean_text) 

    # --- C. æ­£åˆ™åŒ¹é… (æå– æ ‡é¢˜ + é¡µç ) ---
    # åŒ¹é…æ¨¡å¼ï¼šè¡Œé¦–(å¯èƒ½å«ç« èŠ‚å·) + å†…å®¹ + ç©ºæ ¼ + é¡µç (è¡Œå°¾)
    # (?m) å¼€å¯å¤šè¡Œæ¨¡å¼
    # ([^\n\d]+?) åŒ¹é…éæ•°å­—çš„æ ‡é¢˜éƒ¨åˆ† (éè´ªå©ª)
    # (\d+) åŒ¹é…é¡µç 
    pattern = r"(?m)^\s*(.*?)\s+(\d+)\s*$"
    matches = re.findall(pattern, clean_text)

    for title, page_str in matches:
        # æ¸…æ´—æ ‡é¢˜ï¼šå»æ‰é¦–å°¾ç©ºæ ¼ã€å»æ‰æœ«å°¾çš„ç‚¹
        clean_title = title.strip().rstrip('.').rstrip()
        # å»æ‰ä¸­é—´çš„æ‰€æœ‰ç©ºç™½å­—ç¬¦ï¼ˆæ–¹ä¾¿åç»­æ¨¡ç³ŠåŒ¹é…ï¼‰
        compact_title = re.sub(r"\s+", "", clean_title)
        
        # è¿‡æ»¤æ‰è¿‡çŸ­çš„è¯¯åˆ¤ (æ¯”å¦‚åªæœ‰ "1")
        if len(compact_title) > 1:
            try:
                page_num = int(page_str)
                # è¿‡æ»¤æ‰é¡µç å¤§å¾—ç¦»è°±çš„è¯¯åˆ¤
                if page_num <= doc.page_count + 10: 
                    toc_list.append((compact_title, page_num))
            except:
                continue

    # --- D. æ„å»ºé—­ç¯å­—å…¸ {Title: [Start, End]} ---
    toc_dict = {}
    total_items = len(toc_list)
    
    if total_items == 0:
        return {}

    for i in range(total_items):
        title, start_p = toc_list[i]
        
        # ç¡®å®šç»“æŸé¡µï¼šé»˜è®¤ä¸ºä¸‹ä¸€æ¡ç›®çš„å¼€å§‹é¡µ
        if i < total_items - 1:
            next_start_p = toc_list[i+1][1]
            # é€»è¾‘ä¿®æ­£ï¼šæœ‰æ—¶å€™ä¸‹ä¸€ç« å¯èƒ½å’Œå½“å‰ç« åœ¨åŒä¸€é¡µï¼Œæˆ–è€…é¡µç å›æº¯ï¼ˆç›®å½•é¡µç é”™è¯¯ï¼‰
            # æˆ‘ä»¬å– max(start_p, next_start_p) ä¿è¯ä¸å€’é€€
            end_p = max(start_p, next_start_p) 
        else:
            # æœ€åä¸€é¡¹ï¼Œç»“æŸé¡µä¸ºæ–‡æ¡£æ€»é¡µæ•°
            end_p = doc.page_count

        # å­˜å…¥å­—å…¸
        # æ³¨æ„ï¼šå¦‚æœæœ‰é‡åæ ‡é¢˜ï¼ˆæå°‘è§ï¼‰ï¼Œåé¢ä¼šè¦†ç›–å‰é¢ï¼Œæˆ–è€…å¯ä»¥å­˜æˆåˆ—è¡¨
        toc_dict[title] = [start_p, end_p]
    # print("current file toc_dict:")
    # print(toc_dict)

    return toc_dict
# ========================================================
# åŒ¹é…é€»è¾‘ï¼šåœ¨å­—å…¸ä¸­æŸ¥è¡¨
# ========================================================
def match_section_from_dict(toc_dict, keyword, threshold=0.4):
    """
    åœ¨ç›®å½•å­—å…¸ä¸­å¯»æ‰¾æœ€åŒ¹é… keyword çš„æ¡ç›®
    è¿”å›: (start_page, end_page, matched_title)
    """
    if not toc_dict:
        return None, None, None

    best_score = 0
    best_key = None

    for title in toc_dict.keys():
        # 1. å­—ç¬¦è¦†ç›–ç‡ (è§£å†³ "å­˜åœ¨é—®é¢˜" vs "å­˜åœ¨çš„ä¸»è¦é—®é¢˜")
        keyword_chars = set(keyword)
        title_chars = set(title)
        common_chars = keyword_chars.intersection(title_chars)
        coverage = len(common_chars) / len(keyword_chars) if keyword_chars else 0
        
        # 2. åºåˆ—ç›¸ä¼¼åº¦ (difflib)
        seq_score = difflib.SequenceMatcher(None, keyword, title).ratio()
        
        # 3. ç»¼åˆå¾—åˆ†
        # å¦‚æœå…³é”®è¯åŒ…å«åœ¨æ ‡é¢˜é‡Œï¼Œç»™äºˆæé«˜æƒé‡
        if keyword in title:
            final_score = 1.0
        else:
            final_score = max(coverage, seq_score)

        if final_score > best_score:
            best_score = final_score
            best_key = title

    print(f"ğŸ” æœç´¢å…³é”®è¯: '{keyword}' | æœ€ä½³åŒ¹é…: '{best_key}' (å¾—åˆ†: {best_score:.2f})")

    if best_key and best_score >= threshold:
        pages = toc_dict[best_key]
        return pages[0], pages[1], best_key
    else:
        return None, None, None
# ========================================================
# è£å‰ªå‡½æ•°
# ========================================================
def extract_section_to_pdf(pdf_path, output_path, section_keyword="é—®é¢˜"):
    src_doc = None
    out_doc = None
    try:
        src_doc = fitz.open(pdf_path)
        # 1. è®¡ç®—åç§»é‡ (Offset)
        # é€»è¾‘é¡µç  (ç›®å½•ä¸Šçš„ 1) + Offset = ç‰©ç†ç´¢å¼• (FitZ çš„ 4)
        offset = compute_page_offset(src_doc)
        print(f"ğŸ“„ æ–‡æ¡£æ€»é¡µæ•°: {src_doc.page_count}, è®¡ç®—åç§»é‡ Offset = {offset}")
        
        # 2. è§£æç›®å½•
        print("æ­£åœ¨è§£æç›®å½•ç»“æ„...")
        toc_dict = parse_toc_to_dict(src_doc)
        
        if not toc_dict:
            print("âš ï¸ æ–‡æœ¬ç›®å½•è§£æå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ä¹¦ç­¾...")
            # (è¿™é‡Œçœç•¥äº†ä¹¦ç­¾é€»è¾‘ï¼Œå¦‚æœéœ€è¦å¯ä»¥åŠ ä¸Š)
            return False

        # 3. åŒ¹é…åŒºé—´ (å¾—åˆ°çš„æ˜¯ç›®å½•ä¸Šçš„é€»è¾‘é¡µç ï¼Œä¾‹å¦‚ 5 -> 8)
        start_logic, end_logic, matched_title = match_section_from_dict(toc_dict, section_keyword)
        
        if start_logic is None:
            print("âŒ æœªæ‰¾åˆ°åŒ¹é…ç« èŠ‚")
            return False
        
        start_idx = start_logic + offset - 1
        
        # å¤„ç†æœ€åä¸€ç« çš„æƒ…å†µ (end_logic ä¸º 99999)
        if end_logic > 90000:
            end_idx = src_doc.page_count - 1 # ç›´åˆ°æ–‡æ¡£æœ«å°¾
        else:
            end_idx = end_logic + offset - 1
        # 5. è¾¹ç•Œä¿®æ­£
        if start_idx < 0: start_idx = 0
        if start_idx >= src_doc.page_count: 
            print("âŒ è®¡ç®—å‡ºçš„èµ·å§‹é¡µè¶…å‡ºæ–‡æ¡£èŒƒå›´")
            return False
            
        if end_idx >= src_doc.page_count: end_idx = src_doc.page_count - 1
        
        # å…³é”®ä¿®æ­£ï¼šå¦‚æœç®—å‡ºæ¥çš„ end_idx æ¯” start_idx è¿˜å°ï¼ˆç›®å½•é¡µç æ ‡é”™äº†ï¼‰ï¼Œå¼ºåˆ¶å–ä¸€é¡µ
        if end_idx < start_idx: 
            # å°è¯•å¾€åå¤šå–å‡ é¡µï¼Œæ¯”å¦‚é»˜è®¤æå– 3 é¡µ
            print("âš ï¸ ç»“æŸé¡µç å¼‚å¸¸ï¼Œé»˜è®¤æå– 3 é¡µ")
            end_idx = min(start_idx + 2, src_doc.page_count - 1)

        print(f"âœ… æ‰§è¡Œè£å‰ª: {matched_title}")
        print(f"   é€»è¾‘é¡µç : {start_logic} -> {end_logic}")
        print(f"   ç‰©ç†ç´¢å¼•: {start_idx} -> {end_idx} (Offset={offset})")

        out_doc = fitz.open()
        # insert_pdf çš„ to_page æ˜¯åŒ…å«åœ¨å†…çš„ï¼Œæ‰€ä»¥ä¸éœ€è¦ -1
        out_doc.insert_pdf(src_doc, from_page=start_idx, to_page=end_idx)
        out_doc.save(output_path)
        return True

    except Exception as e:
        print(f"è£å‰ªè¿‡ç¨‹å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        return False
    finally:
        if src_doc: src_doc.close()
        if out_doc: out_doc.close()



def open_pdf_auto_repair(pdf_path):
    """
    å°è¯•æ‰“å¼€ PDF çš„é€šç”¨å·¥å…·å‡½æ•°ã€‚
    ä¼˜å…ˆå°è¯• fitz ç›´æ¥æ‰“å¼€ï¼Œå¤±è´¥åˆ™è°ƒç”¨ pikepdf ä¿®å¤æµã€‚
    """
    try:
        return fitz.open(pdf_path)
    except Exception as e:
        # print(f"fitz æ‰“å¼€å¤±è´¥: {e}ï¼Œå°è¯•ä¿®å¤...")
        try:
            with pikepdf.open(pdf_path, allow_overwriting_input=True) as p:
                mem_stream = io.BytesIO()
                p.save(mem_stream)
                mem_stream.seek(0)
                return fitz.open("pdf", mem_stream)
        except Exception:
            return None

def compute_page_offset(pdf_path, max_pages_to_check=20):
    """è®¡ç®—é¡µç åç§»é‡"""
    doc = None
    try:
        doc = open_pdf_auto_repair(pdf_path)
        if doc is None: return 0

        for i in range(min(max_pages_to_check, doc.page_count)):
            try:
                page = doc[i]
                text = page.get_text()
                if not text: continue
                lines = [line.strip() for line in text.split("\n") if line.strip()]
                if not lines: continue
                
                last_line = lines[-1]
                match = re.search(r"(?:ç¬¬\s*(\d+)\s*é¡µ)|^\s*(\d+)\s*$", last_line)
                if match:
                    logical_page = int(match.group(1)) if match.group(1) else int(match.group(2))
                    offset = i - (logical_page - 1)
                    return offset
            except:
                continue
    except:
        return 0
    finally:
        if doc: doc.close()
    return 0

'''
def find_section_pages(pdf_path, section_title="é—®é¢˜"):
    """æŸ¥æ‰¾ç« èŠ‚èµ·æ­¢é¡µç """
    start_page, end_page = None, None
    doc = None

    try:
        doc = open_pdf_auto_repair(pdf_path)
        if doc is None: return None, None

        toc_text = ""
        # æ‰«æå‰ 20 é¡µä½œä¸ºç›®å½•
        for i in range(min(20, doc.page_count)):
            try:
                page_text = doc[i].get_text()
                if page_text: toc_text += page_text + "\n"
            except: continue
        clean_toc_text = re.sub(r"[â€¦\.ï¼]{2,}", " ", toc_text)
        clean_toc_text = re.sub(r'(?m)^\s*([ï¼ˆ(]?\s*[\dä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å][\d.ï¼)ï¼‰]*[ã€]?)\s*\n', r'\1 ', clean_toc_text)
        pattern = r"(?m)^\s*([ï¼ˆ(]?\s*[\dä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å].*?)\s+(\d+)\s*$"
        matches = re.findall(pattern, clean_toc_text)

        toc = []
        for title, page in matches:
            clean_title = title.strip().rstrip('.').rstrip()
            compact_title = re.sub(r"\s+", "", clean_title)
            toc.append((compact_title, int(page)))
            
        for idx, (title, page) in enumerate(toc):
            if section_title in title:
                start_page = page
                found_valid_end = False
                for next_idx in range(idx + 1, len(toc)):
                    candidate_end = toc[next_idx][1]
                    if candidate_end > start_page:
                        end_page = candidate_end
                        found_valid_end = True
                        break 
                if not found_valid_end:
                    end_page = doc.page_count + 1
                break 
                
    except:
        return None, None
    finally:
        if doc: doc.close()

    return start_page, end_page
'''

'''
def extract_section_to_pdf(pdf_path, output_path, section_title="é—®é¢˜"):
    """æ‰§è¡Œè£å‰ªä¸»é€»è¾‘"""
    src_doc = None
    out_doc = None
    try:
        offset = compute_page_offset(pdf_path)
        start_logic, end_logic = find_section_pages(pdf_path, section_title)
        
        if start_logic is None or end_logic is None:
            return False

        src_doc = open_pdf_auto_repair(pdf_path)
        if not src_doc: return False

        start_idx = start_logic + offset - 1
        end_idx = end_logic + offset - 1
        
        # è¾¹ç•Œä¿®æ­£
        if start_idx < 0: start_idx = 0
        if end_idx > src_doc.page_count: end_idx = src_doc.page_count
        if start_idx >= end_idx: return False

        out_doc = fitz.open()
        out_doc.insert_pdf(src_doc, from_page=start_idx, to_page=end_idx - 1)
        out_doc.save(output_path)
        return True
    except:
        return False
    finally:
        if src_doc: src_doc.close()
        if out_doc: out_doc.close()     
'''

def extract_section_to_pdf_self(pdf_path, start, end, output_path):
    """
    æŒ‰æŒ‡å®šé¡µç è£å‰ª PDF å¹¶ä¿å­˜ (PyMuPDF å¢å¼ºç‰ˆ)
    start/end: é€»è¾‘é¡µç  (ä» 1 å¼€å§‹)
    end: ç»“æŸé¡µç  (ä¸åŒ…å«ï¼Œä¸ Python range ä¹ æƒ¯ä¸€è‡´ï¼Œä¾‹å¦‚ start=1, end=3 æå–ç¬¬1,2é¡µ)
         (æ³¨æ„ï¼šè¯·ç¡®è®¤æ‚¨çš„è°ƒç”¨é€»è¾‘ï¼Œå¦‚æœ end æ˜¯åŒ…å«çš„ï¼Œè¯·åœ¨ä¸‹æ–¹ indices è®¡ç®—æ—¶è°ƒæ•´)
    """
    offset = 0  # é»˜è®¤ä¸åç§»ï¼Œå¦‚æœéœ€è¦è‡ªåŠ¨è®¡ç®—åç§»ï¼Œå¯è°ƒç”¨ compute_page_offset
    src_doc = None
    out_doc = None
    
    try:
        # 1. ä½¿ç”¨è‡ªåŠ¨ä¿®å¤åŠŸèƒ½æ‰“å¼€æºæ–‡ä»¶
        src_doc = open_pdf_auto_repair(pdf_path)
        if not src_doc:
            print(f"âŒ æ— æ³•æ‰“å¼€æˆ–ä¿®å¤æ–‡ä»¶: {pdf_path}")
            return False

        # 2. è½¬æ¢é¡µç ä¸ºç‰©ç†ç´¢å¼• (0-based)
        # ç”¨æˆ·ä¼ å…¥çš„ start æ˜¯ 1-basedï¼Œæ‰€ä»¥å‡ 1
        start_idx = start + offset - 1
        # ç”¨æˆ·ä¼ å…¥çš„ end æ˜¯ 1-based ä¸”é€šå¸¸ä½œä¸º range çš„ç»“å°¾ (exclusive)ï¼Œæ‰€ä»¥å‡ 1
        end_idx = end + offset - 1
        
        # è¾¹ç•Œæ£€æŸ¥
        if start_idx < 0: start_idx = 0
        if end_idx > src_doc.page_count: end_idx = src_doc.page_count
        
        if start_idx >= end_idx:
            print(f"âš  é¡µç èŒƒå›´æ— æ•ˆæˆ–ä¸ºç©º: {start}-{end} (Indices: {start_idx}-{end_idx})")
            return False

        # 3. æå–å¹¶ä¿å­˜
        out_doc = fitz.open()
        
        # fitz.insert_pdf çš„å‚æ•° from_page æ˜¯åŒ…å«çš„ï¼Œto_page ä¹Ÿæ˜¯åŒ…å«çš„
        # æˆ‘ä»¬è¦æå– [start_idx, end_idx) åŒºé—´
        # æ‰€ä»¥ to_page åº”è¯¥æ˜¯ end_idx - 1
        out_doc.insert_pdf(src_doc, from_page=start_idx, to_page=end_idx - 1)
        
        out_doc.save(output_path)
        print(f"è‡ªå®šä¹‰å¤„ç†å®Œæˆ -> {os.path.basename(output_path)}")
        return True

    except Exception as e:
        print(f"âŒ è‡ªå®šä¹‰æå–å¤±è´¥: {e}")
        return False
    finally:
        # ç¡®ä¿å…³é—­æ–‡ä»¶å¥æŸ„
        if src_doc: src_doc.close()
        if out_doc: out_doc.close()

def parser_file(filename):
    """
    è§£ææ–‡ä»¶åï¼Œè¿”å›å­—å…¸ã€‚
    """
    city = "æœªçŸ¥åŸå¸‚"
    district = "-"
    unit = ""
    
    # 1. å¼ºåŠ›æ¸…æ´—ï¼šå»æ‰ .pdf å’Œæ‰€æœ‰å¯èƒ½çš„ä»»åŠ¡åç¼€
    clean_name = filename.replace(".pdf", "")
    
    if '_' in clean_name:
        clean_name = clean_name.split('_')[0]
    
    # === ç­–ç•¥ A: å¤„ç†çŸ­æ¨ªçº¿æ ¼å¼ (City-District) ===
    if '-' in clean_name:
        parts = clean_name.split('-')
        if len(parts) >= 1: city = parts[0]
        if len(parts) >= 2: district = parts[1]
        if len(parts) >= 3: unit = parts[2]
            
        return {
            "åŸå§‹æ–‡ä»¶å": filename,
            "æ–‡ä»¶å": clean_name,  # ç›´æ¥ä½¿ç”¨æ¸…æ´—åçš„åå­—
            "åŸå¸‚": city,
            "åœ°åŒº/å¿": district,
            "è¯¦ç»†å•å…ƒ": unit if unit else "æ— "
        }
        
def extract_info(filename):
    """
    è§£ææ–‡ä»¶åï¼Œè¿”å›å­—å…¸ã€‚
    å…¼å®¹ä¸¤ç§æ¨¡å¼ï¼š
    1. å·²æ¸…æ´—è¿‡çš„æ ¼å¼ï¼š'ä¸œè-å‡¤å²—_landuse.pdf' -> æå–ä¸º ä¸œè, å‡¤å²—
    2. åŸå§‹é•¿æ–‡ä»¶åï¼š'ä¸œèå¸‚å‡¤å²—é•‡å…¨åŸŸ...pdf' -> æå–ä¸º ä¸œèå¸‚, å‡¤å²—é•‡
    """
    city = "æœªçŸ¥åŸå¸‚"
    district = "-" 
    unit = ""
    
    # 1. åŸºç¡€æ¸…æ´—ï¼šå»æ‰ .pdf
    clean_name = filename.replace(".pdf", "")
    
    # 2. å‰¥ç¦»ä»»åŠ¡åç¼€ (è¿™æ˜¯å…³é”®ï¼æŠŠ _landuse, _issue ç­‰å»æ‰ï¼Œè¿˜åŸæˆ åœ°åŒº-åŒºå¿)
    # æ­£åˆ™è§£é‡Šï¼šåŒ¹é…ä¸‹åˆ’çº¿å¼€å¤´ï¼Œåé¢è·Ÿç€ä»»åŠ¡åï¼Œç›´åˆ°å­—ç¬¦ä¸²ç»“æŸ
    clean_name = re.sub(r'(_landuse|_issue|_potential|_project|_spatial|_data|_cropped|_manual).*$', '', clean_name)

    # === ç­–ç•¥ A: å¤„ç†å·²ç»æ¸…æ´—è¿‡çš„çŸ­æ¨ªçº¿æ ¼å¼ (City-District-Unit) ===
    # å¦‚æœåå­—é‡Œæœ‰æ¨ªçº¿ï¼Œè¯´æ˜è¿™æ˜¯æˆ‘ä»¬è‡ªå·±ç”Ÿæˆçš„æ–‡ä»¶ï¼Œç›´æ¥åˆ‡åˆ†å³å¯
    if '-' in clean_name:
        parts = clean_name.split('-')
        # åªæœ‰ä¸€æ®µï¼š 'ä¸œè'
        if len(parts) >= 1:
            city = parts[0]
        # æœ‰ä¸¤æ®µï¼š 'ä¸œè-å‡¤å²—'
        if len(parts) >= 2:
            district = parts[1]
        # æœ‰ä¸‰æ®µï¼š 'ä¸œè-å‡¤å²—-å®˜äº•å¤´'
        if len(parts) >= 3:
            unit = parts[2]
            
        return {
            "åŸå§‹æ–‡ä»¶å": filename,
            "æ–‡ä»¶å": clean_name, # ç”¨äºæ˜¾ç¤ºçš„çº¯åœ°åŒºå
            "åŸå¸‚": city,
            "åœ°åŒº/å¿": district if district else "-",
            "è¯¦ç»†å•å…ƒ": unit if unit else "æ— "
        }

    # === ç­–ç•¥ B: å¤„ç†åŸå§‹é•¿æ–‡ä»¶å (Regex åŒ¹é…) ===
    # åŒ¹é…è§„åˆ™ï¼šä»¥"å¸‚"ç»“å°¾çš„å‰ç¼€ + ä¸­é—´åŒºåŸŸå + å…³é”®è¯
    match = re.search(r'^(.+?å¸‚)(.+?)(?:å…¨åŸŸ|å®æ–½|é¡¹ç›®|æ°¸ä¹…|åœŸåœ°)', clean_name)
    if match:
        city = match.group(1)
        district = match.group(2)
    else:
        # ç‰¹æ®Šè§„åˆ™å…œåº•
        if "å¹¿å·å¸‚-æ¹›æ±Ÿå¸‚" in filename:
            city = "å¹¿å·æ¹›æ±Ÿåˆä½œå›­"
            district = "å¥‹å‹‡é«˜æ–°åŒº"
        elif "å¸‚" in clean_name and city == "æœªçŸ¥åŸå¸‚":
            # æœ€åçš„å°è¯•ï¼šæŒ‰â€œå¸‚â€å­—åˆ‡åˆ†
            try:
                idx = clean_name.index("å¸‚")
                city = clean_name[:idx+1]
                district = clean_name[idx+1:]
            except: pass
    # æå–æ‹¬å·å†…å®¹
    unit_match = re.search(r'[ï¼ˆ\(](.+?)[ï¼‰\)]', filename)
    if unit_match:
        unit = unit_match.group(1)
    
    # --- æ•°æ®æ¸…æ´—ä¸æ ¼å¼åŒ– (é’ˆå¯¹åŸå§‹æ–‡ä»¶å) ---
    short_city = city.replace("å¸‚", "")
    short_district = district
    
    # æ¸…æ´—åŒºå¿åç¼€
    for suffix in ["å¸‚", "åŒº", "å¿", "é•‡", "è¡—é“", "è‡ªæ²»å¿", "æ–°åŒº", "ç®¡ç†åŒº", "å¼€å‘åŒº", "ç‰¹åˆ«åˆä½œåŒº"]:
        if short_district.endswith(suffix) and len(short_district) > len(suffix):
            if short_district == "å—åŒº" and suffix == "åŒº": continue
            short_district = short_district.replace(suffix, "")
            break
            
    short_unit = unit
    for suffix in ["å®æ–½å•å…ƒ", "å•å…ƒ", "é•‡", "è¡—é“", "ç‰‡åŒº", "å®æ–½æ–¹æ¡ˆ"]:
        short_unit = short_unit.replace(suffix, "")
    
    # ç»„è£…æ ‡å‡†åŒ–åå­—
    components = [short_city]
    if short_district and short_district != "-": components.append(short_district)
    if short_unit: components.append(short_unit)
    
    new_name = "-".join(components)

    return {
        "åŸå§‹æ–‡ä»¶å": filename,
        "æ–‡ä»¶å": new_name,
        "åŸå¸‚": city,
        "åœ°åŒº/å¿": short_district if short_district else "-",
        "è¯¦ç»†å•å…ƒ": unit if unit else "æ— "
    }
    
    
def extract_pages_by_keywords(pdf_path, output_path, keyword_pattern_str):
    """
    æ‰«ææ¯ä¸€é¡µå†…å®¹ï¼ŒåŒ¹é…å…³é”®è¯ï¼ˆæ”¯æŒæ­£åˆ™è¡¨è¾¾å¼ï¼‰ã€‚
    å¦‚æœæ‰¾åˆ°æ ‡é¢˜ï¼Œä¸”åç»­é¡µé¢æ˜¯è¿ç»­è¡¨æ ¼ï¼Œä¼šè‡ªåŠ¨åˆå¹¶åç»­é¡µé¢ã€‚
    """
    pages_to_save = []
    in_table = False
    
    try:
        search_pattern = re.compile(keyword_pattern_str)
    except:
        # å¦‚æœç”¨æˆ·è¾“å…¥çš„ä¸æ˜¯æ­£åˆ™ï¼Œè½¬ä¸ºæ™®é€šåŒ…å«åŒ¹é…
        search_pattern = re.compile(re.escape(keyword_pattern_str))

    src_doc = None
    out_doc = None
    
    try:
        src_doc = open_pdf_auto_repair(pdf_path)
        if not src_doc: return False
        
        for page_index, page in enumerate(src_doc):
            text = page.get_text() or ""
            
            # åˆ¤æ–­æ˜¯å¦åŒ…å«æ ‡é¢˜
            has_title = bool(search_pattern.search(text))
            
            # åˆ¤æ–­æ˜¯å¦æœ‰è¡¨æ ¼ (PyMuPDF åŠŸèƒ½)
            # find_tables æ¯”è¾ƒè€—æ—¶ï¼Œä»…åœ¨å¿…è¦æ—¶è°ƒç”¨æˆ–æ¯ä¸€é¡µè°ƒç”¨
            tables = page.find_tables()
            has_table = len(tables.tables) > 0
            
            if has_title:
                in_table = True
                pages_to_save.append(page_index)
            elif in_table and has_table:
                # å¦‚æœå¤„äº"è¡¨æ ¼è¿ç»­æ¨¡å¼"ä¸”å½“å‰é¡µä¹Ÿæœ‰è¡¨æ ¼ï¼Œåˆ¤å®šä¸ºè·¨é¡µè¡¨æ ¼
                pages_to_save.append(page_index)
            else:
                # æ–­å¼€è¿ç»­
                in_table = False
        
        if not pages_to_save:
            return False
            
        # ä¿å­˜ç»“æœ
        out_doc = fitz.open()
        for p_idx in pages_to_save:
            out_doc.insert_pdf(src_doc, from_page=p_idx, to_page=p_idx)
            
        out_doc.save(output_path)
        return True

    except Exception as e:
        print(f"å…³é”®è¯æå–å¤±è´¥: {e}")
        return False
    finally:
        if src_doc: src_doc.close()
        if out_doc: out_doc.close()


        
def dict_save2csv(data: dict, save_path: str):
    """
    å°†å­—å…¸æ•°æ®ä¿å­˜ä¸º CSV æ–‡ä»¶
    """
    import pandas as pd
    df = pd.DataFrame.from_dict(data, orient='index')
    df.reset_index(inplace=True)
    df.rename(columns={'index': 'åœ°åŒº'}, inplace=True)
    df.to_csv(save_path, index=False, encoding='utf-8-sig')
    print(f"æ•°æ®å·²ä¿å­˜åˆ° {save_path}")
