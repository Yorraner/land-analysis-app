import streamlit as st
import os
import io
import pandas as pd
import numpy as np
import time
import json
import zipfile
import time
import shutil
from utils.utils_pdf import extract_section_to_pdf, extract_section_to_pdf_self, \
    extract_info,parser_file,extract_pages_by_keywords,dict_save2csv
from utils.api_client import CozeClient, get_mock_data, WORKFLOW_CONFIG 
from utils.utils_fusion import unify_and_concatenate, preprocess_X 
from utils.utils_vis import plot_heatmap ,plot_horizontal_bars_from_df,plot_category_radar_chart,plot_clusters
from utils.utils_parse import process_raw_data
from utils.algorithm import clustering_kmeans_with_entropy_expert,build_weight_vector
from utils.login import check_password

# from utils_parsers import process_raw_data
# from utils_fusion import unify_and_concatenate
bg_path = "imgs/bg1.png"
json_path="./users.json"
# === login ===
if not check_password(bg_path,json_path):
    st.stop()
# === é¡µé¢é…ç½® ===
st.set_page_config(page_title="åœŸåœ°æ•´æ²»æ™ºèƒ½åˆ†æå¹³å°", layout="wide")
st.title("ğŸ—ï¸ åœŸåœ°æ•´æ²»æ–‡æ¡£æ™ºèƒ½åˆ†ç±»ç³»ç»Ÿ")

# === ä¸´æ—¶æ–‡ä»¶ç®¡ç† ===
TEMP_DIR = "temp_workspace"
DIRS = {
    "upload": os.path.join(TEMP_DIR, "1_uploads"),
    "crop": os.path.join(TEMP_DIR, "2_cropped"),
    "raw": os.path.join(TEMP_DIR, "3_raw_data"),
    "result": os.path.join(TEMP_DIR, "4_results"), 
    "final": os.path.join(TEMP_DIR, "5_final")
}
TEMPLATE_COLUMNS = {
    "spatial": ["åœ°åŒº", "æ°¸å†œè°ƒå…¥è§„æ¨¡ï¼ˆå…¬é¡·ï¼‰", "æ°¸å†œè°ƒå‡ºè§„æ¨¡ï¼ˆå…¬é¡·ï¼‰", "åŸé•‡å¼€å‘è°ƒå…¥è§„æ¨¡ï¼ˆå…¬é¡·ï¼‰", "åŸé•‡å¼€å‘è°ƒå‡ºè§„æ¨¡ï¼ˆå…¬é¡·ï¼‰", "è§„åˆ’å•å…ƒç©ºé—´è°ƒæ•´æ‰“åˆ†ï¼ˆæœ€é«˜5åˆ†ï¼‰"],
    "potential": ["åœ°åŒº", "å¦é€ æ°´ç”°æ½œåŠ›", "æ–°å¢è€•åœ°æ½œåŠ›", "è€•åœ°â€œéç²®åŒ–â€æ•´æ²»æ½œåŠ›","è€•åœ°æ¢å¤æ½œåŠ›", "é«˜æ ‡å‡†å†œç”°å»ºè®¾æ½œåŠ›","è€•åœ°æè´¨æ”¹é€ æ½œåŠ›",
                  "è¡¥å……è€•åœ°æ½œåŠ›","è€•åœ°é›†ä¸­æ•´æ²»åŒºå»ºè®¾æ½œåŠ›","ä½æ•ˆå·¥ä¸šç”¨åœ°è…¾é€€æ½œåŠ›","å­˜é‡ä½æ•ˆç”¨åœ°æ½œåŠ›","çŸ¿å±±ä¿®å¤æ½œåŠ›", "çº¢æ ‘æ—ä¿æŠ¤æ½œåŠ›","ä½æ•ˆç”¨åœ°å†å¼€å‘æ½œåŠ›",
                  "ä¸‰æ—§æ”¹é€ æ½œåŠ›","åŸé•‡ä½æ•ˆç”¨åœ°å†å¼€å‘æ½œåŠ›","å»ºè®¾ç”¨åœ°å¢å‡æŒ‚é’©ï¼ˆæ‹†æ—§å¤å¦ï¼‰æ½œåŠ›",	"åœŸå£¤æ•´æ²»æ½œåŠ›","é€ æ—ç»¿åŒ–æ½œåŠ›","æµåŸŸç”Ÿæ€ä¿®å¤æ½œåŠ›"],
    "issue": ["åœ°åŒº", "è€•åœ°ç¢ç‰‡åŒ–", "äº§ä¸šå‘å±•ä¸ç”¨åœ°ä¾›ç»™çŸ›ç›¾","äººåœ°åè°ƒçŸ›ç›¾","äººä¸è‡ªç„¶çš„çŸ›ç›¾","ä½æ•ˆç”¨åœ°é—®é¢˜"],
    "LandUse": ["åœ°åŒº", "å†œç”¨åœ°", "å»ºè®¾ç”¨åœ°", "ç”Ÿæ€ä¿æŠ¤", "æ—åœ°å æ¯”"],
    "project": ["åœ°åŒº", "å†œç”¨åœ°æ•´ç†ç±»é¡¹ç›®_æ•°é‡", "å†œç”¨åœ°æ•´ç†ç±»é¡¹ç›®_æŠ•èµ„", "å†œç”¨åœ°æ•´ç†ç±»é¡¹ç›®_è§„æ¨¡",
                "å»ºè®¾ç”¨åœ°æ•´ç†ç±»é¡¹ç›®_æ•°é‡", 	"å»ºè®¾ç”¨åœ°æ•´ç†ç±»é¡¹ç›®_æŠ•èµ„", 	"å»ºè®¾ç”¨åœ°æ•´ç†ç±»é¡¹ç›®_è§„æ¨¡", 	
                "ç”Ÿæ€ä¿æŠ¤ä¿®å¤ç±»é¡¹ç›®_æ•°é‡", 	"ç”Ÿæ€ä¿æŠ¤ä¿®å¤ç±»é¡¹ç›®_æŠ•èµ„", 	"ç”Ÿæ€ä¿æŠ¤ä¿®å¤ç±»é¡¹ç›®_è§„æ¨¡", 	
                "ä¹¡æ‘é£è²Œæå‡å’Œå†å²æ–‡åŒ–ä¿æŠ¤ç±»é¡¹ç›®_æ•°é‡", "ä¹¡æ‘é£è²Œæå‡å’Œå†å²æ–‡åŒ–ä¿æŠ¤ç±»é¡¹ç›®_æŠ•èµ„", "ä¹¡æ‘é£è²Œæå‡å’Œå†å²æ–‡åŒ–ä¿æŠ¤ç±»é¡¹ç›®_è§„æ¨¡"	, 
                "å…¬å…±æœåŠ¡ä¸åŸºç¡€è®¾æ–½å»ºè®¾ç±»é¡¹ç›®_æ•°é‡"	"å…¬å…±æœåŠ¡ä¸åŸºç¡€è®¾æ–½å»ºè®¾ç±»é¡¹ç›®_æŠ•èµ„", "å…¬å…±æœåŠ¡ä¸åŸºç¡€è®¾æ–½å»ºè®¾ç±»é¡¹ç›®_è§„æ¨¡"	, 
                "äº§ä¸šå¯¼å…¥ç±»é¡¹ç›®_æ•°é‡", 	"äº§ä¸šå¯¼å…¥ç±»é¡¹ç›®_æŠ•èµ„", 	"äº§ä¸šå¯¼å…¥ç±»é¡¹ç›®_è§„æ¨¡", 
                "å…¶ä»–ç±»é¡¹ç›®_æ•°é‡", 	"å…¶ä»–ç±»é¡¹ç›®_æŠ•èµ„", 	"å…¶ä»–ç±»é¡¹ç›®_è§„æ¨¡", ],
    "default": ["åœ°åŒº", "æŒ‡æ ‡1", "æŒ‡æ ‡2", "æŒ‡æ ‡3"]
}


# åˆå§‹åŒ–ç›®å½•
for d in DIRS.values():
    if not os.path.exists(d): os.makedirs(d)

# === ä¾§è¾¹æ ï¼šæµç¨‹æ§åˆ¶ ===
with st.sidebar:
    st.header("å·¥ä½œæµå¯¼èˆª")
    step = st.radio("é€‰æ‹©æ­¥éª¤", [
        "1. æ–‡æ¡£ä¸Šä¼ ä¸è£å‰ª", 
        "2. å¤§æ¨¡å‹æ•°æ®è·å–", 
        "3. æ•°æ®è§£æ", 
        "4. æ•°æ®èåˆ&å±•ç¤º",
        "5. æ•°æ®åˆ†ç±»ä¸å¯¼å‡º"
    ])
    st.divider()
    if st.button("æ¸…ç†ä¸´æ—¶æ–‡ä»¶"):
        if os.path.exists(TEMP_DIR):
            shutil.rmtree(TEMP_DIR)
            for d in DIRS.values():
                if not os.path.exists(d): os.makedirs(d)
        st.success("å·²æ¸…ç†ç¼“å­˜")

# å®šä¹‰å…¨å±€ä»»åŠ¡å­—å…¸
TASK_DICT={
    "è‡ªç„¶èµ„æºç¦€èµ‹":"LandUse",
    "å­˜åœ¨é—®é¢˜":"issue",
    "æ•´æ²»æ½œåŠ›":"potential",
    "å­é¡¹ç›®":"project",
    "ç©ºé—´å¸ƒå±€":"spatial"
}

def render_file_manager(dir_path, title="ç»“æœæ–‡ä»¶ç®¡ç†", file_ext=".csv", key_prefix="common"):
    """
    é€šç”¨æ–‡ä»¶ç®¡ç†ç»„ä»¶ï¼šåˆ—è¡¨ã€é¢„è§ˆã€ä¸‹è½½ã€åˆ é™¤
    """
    st.divider()
    st.subheader(f"ğŸ“‚ {title}")
    
    if not os.path.exists(dir_path):
        st.info("æš‚æ— æ–‡ä»¶ç”Ÿæˆã€‚")
        return
    # scan files
    # files = [f for f in os.listdir(dir_path) if f.endswith(file_ext)]
    files =  os.listdir(dir_path)
    files.sort(key=lambda x: os.path.getmtime(os.path.join(dir_path, x)), reverse=True) # æŒ‰æ—¶é—´å€’åº

    if files:
        # 1. file table displayview
        view_files = files
        if  key_prefix =="step3":
            view_files = [i for i in files if i.startswith('parsed') and not i.endswith("matrix.csv")]
        elif  key_prefix =="step4":
            view_files = [i for i in files if i.startswith('fusion')]
        df_files = pd.DataFrame(view_files, columns=["æ–‡ä»¶å"],index=range(1, len(view_files)+1))
        st.dataframe(df_files, width="stretch", height=150)
        
        # 2. file delete
        with st.expander("ğŸ—‘ï¸ ç®¡ç†/åˆ é™¤æ–‡ä»¶"):
            files_to_del = st.multiselect("é€‰æ‹©è¦åˆ é™¤çš„æ–‡ä»¶", files, key=f"{key_prefix}_del_multi")
            if st.button("ç¡®è®¤åˆ é™¤", key=f"{key_prefix}_del_btn"):
                for f in files_to_del:
                    try: os.remove(os.path.join(dir_path, f))
                    except: pass
                st.success(f"å·²åˆ é™¤ {len(files_to_del)} ä¸ªæ–‡ä»¶")
                time.sleep(1)
                st.rerun()

        # 3. preview & single download
        c1, c2 = st.columns([2, 1])
        with c1:
            select_files = files
            if  key_prefix =="step3":
                select_files = [i for i in files if i.startswith('parsed') ]
            elif  key_prefix =="step4":
                select_files = [i for i in files if not i.startswith('parsed')]
            sel_file = st.selectbox("é€‰æ‹©æ–‡ä»¶é¢„è§ˆ:", select_files, key=f"{key_prefix}_sel")
            if sel_file:
                file_path = os.path.join(dir_path, sel_file)
                file_ext = os.path.splitext(sel_file)[1].lower()
                if file_ext == ".csv":
                    try:
                        try: df = pd.read_csv(file_path)
                        except: df = pd.read_csv(file_path, encoding='gbk')
                        st.write(f"ğŸ“Š `{sel_file}` :")
                        df_preview = df.head()
                        df_preview.index = df_preview.index+1
                        st.dataframe(df_preview)
                    except Exception as e:
                        st.error(f"è¯»å–å¤±è´¥: {e}")
                elif file_ext == ".png":
                    st.caption("png æ–‡ä»¶ä¸æ”¯æŒç›´æ¥é¢„è§ˆï¼Œè¯·ä¸‹è½½æŸ¥çœ‹ã€‚")
        with c2:
            if sel_file:
                file_path = os.path.join(dir_path, sel_file)
                with open(file_path, "rb") as f:
                    mime_type = "text/csv" if file_ext == ".csv" else "application/pdf"
                    st.download_button(
                        label=f"ğŸ“¥ ä¸‹è½½ \
                        \n{sel_file}",
                        data=f,
                        file_name=sel_file,
                        mime=mime_type,
                        key=f"{key_prefix}_down_btn",
                        type="primary"
                    )           
        # 4. package download
        st.caption("ğŸ“¦ **æ‰¹é‡ä¸‹è½½**")
        c_dl1, c_dl2 = st.columns([2, 1])
        with c_dl1:
            # 1. æä¾›æ–‡ä»¶ç±»å‹é€‰æ‹© (æ ¹æ®å½“å‰ç›®å½•ä¸‹çš„å®é™…æ–‡ä»¶åç¼€è‡ªåŠ¨ç”Ÿæˆé€‰é¡¹)
            # è·å–ç›®å½•ä¸‹æ‰€æœ‰æ–‡ä»¶çš„åç¼€
            available_exts = list(set([f.split('.')[-1] for f in files if '.' in f]))
            # è®¾ç½®é»˜è®¤é€‰ä¸­çš„ç±»å‹
            default_exts = []
            if "csv" in available_exts: default_exts.append("csv")
            # if "png" in available_exts: default_exts.append("png") # å¦‚æœæœ‰å›¾ï¼Œé»˜è®¤ä¹Ÿå‹¾é€‰
            selected_exts = st.multiselect(
                "é€‰æ‹©è¦æ‰“åŒ…çš„æ–‡ä»¶ç±»å‹:",
                options=available_exts,
                default=default_exts,
                key=f"{key_prefix}_ext_sel"
            )
        with c_dl2:
            # 2. æ ¹æ®æ­¥éª¤ (Key Prefix) å’Œ é€‰ä¸­çš„ç±»å‹ è¿›è¡ŒåŒé‡è¿‡æ»¤
            files_to_zip = []
            
            for f in files:
                # è·å–æ–‡ä»¶åç¼€
                f_ext = f.split('.')[-1]
                if f_ext in selected_exts:
                    if key_prefix == "step3":
                        # Step 3 åªä¸‹è½½ä»¥ 'parsed' å¼€å¤´çš„æ–‡ä»¶
                        if f.startswith('parsed'):
                            files_to_zip.append(f)
                    
                    elif key_prefix == "step4":
                        files_to_zip.append(f)
                        
                    elif key_prefix == "step5":
                        files_to_zip.append(f)
                    
                    else:
                        # å…¶ä»–æƒ…å†µï¼Œåªè¦åç¼€åŒ¹é…å°±åŠ å…¥
                        files_to_zip.append(f)

            # 3. æ‰§è¡Œæ‰“åŒ…
            zip_name = f"selected_{key_prefix}_files.zip"
            zip_path = os.path.join(TEMP_DIR, zip_name) # ç¡®ä¿ TEMP_DIR å·²å®šä¹‰

            # åªæœ‰å½“æœ‰æ–‡ä»¶è¢«é€‰ä¸­æ—¶æ‰æ˜¾ç¤ºä¸‹è½½æŒ‰é’®
            if files_to_zip:
                # åˆ›å»º ZIP
                with zipfile.ZipFile(zip_path, 'w') as zf:
                    for f in files_to_zip:
                        # dir_path æ˜¯ä¼ å…¥è¯¥å‡½æ•°çš„å½“å‰ç›®å½•è·¯å¾„
                        full_path = os.path.join(dir_path, f)
                        zf.write(full_path, f)
                
                # æ˜¾ç¤ºæŒ‰é’®
                with open(zip_path, "rb") as f:
                    st.download_button(
                        f"â¬‡ï¸ ä¸‹è½½é€‰ä¸­æ–‡ä»¶ ({len(files_to_zip)}ä¸ª)", 
                        f, 
                        zip_name, 
                        "application/zip", 
                        key=f"{key_prefix}_zip",
                        type="primary"
                    )
            else:
                st.caption("âš ï¸ æ²¡æœ‰åŒ¹é…çš„æ–‡ä»¶å¯ä¸‹è½½")
        
    
    else:
        st.info(f"å½“å‰ä»»åŠ¡çš„ç›®å½•ä¸ºç©º ({dir_path})")
# ========================================================
# 1. ä¸Šä¼ ä¸è£å‰ª
# ========================================================
if step == "1. æ–‡æ¡£ä¸Šä¼ ä¸è£å‰ª":
    st.header("ğŸ“„ æ­¥éª¤ 1: PDF æ–‡æ¡£å¤„ç†")
    tab1, tab2 = st.tabs(["ğŸš€ æ‰¹é‡è‡ªåŠ¨è£å‰ª", "ğŸ› ï¸ æ‰‹åŠ¨è£å‰ªä¿®å¤"])
    with tab1:
        st.markdown("ä¸Šä¼ åŸå§‹æ–‡æ¡£ï¼Œç³»ç»Ÿå°†æ ¹æ®æå–æ¨¡å¼è‡ªåŠ¨è£å‰ªå‡ºå…³é”®é¡µé¢ã€‚")
        st.info("ğŸ’¡ æç¤ºï¼šé»˜è®¤æ”¯æŒæœ€å¤§ 1GB æ–‡ä»¶ã€‚å»ºè®®åˆ†æ‰¹ä¸Šä¼ ã€‚")
        source_option = st.radio("é€‰æ‹©æ–‡ä»¶æ¥æº", ["ğŸ“¤ ä¸Šä¼ æ–°æ–‡ä»¶", "ğŸ“‚ ä½¿ç”¨æœåŠ¡å™¨å·²å­˜åœ¨æ–‡ä»¶"])
        target_files = [] 
        if source_option == "ğŸ“¤ ä¸Šä¼ æ–°æ–‡ä»¶":
            uploaded_files = st.file_uploader(
                "ä¸Šä¼  PDF æ–‡ä»¶", 
                type=["pdf"], 
                accept_multiple_files=True, 
                key="auto_uploader"
            )
            
            if uploaded_files:
                save_status = st.empty()
                save_status.text("æ­£åœ¨ä¿å­˜æ–‡ä»¶åˆ°ç¡¬ç›˜...")
                
                saved_count = 0
                for f in uploaded_files:
                    file_path = os.path.join(DIRS["upload"], f.name)
                    with open(file_path, "wb") as buffer:
                        buffer.write(f.getbuffer()) 
                    saved_count += 1
                    target_files.append(file_path)
                save_status.success(f"âœ… å·²ä¿å­˜ {saved_count} ä¸ªæ–‡ä»¶åˆ°æœåŠ¡å™¨ç¼“å­˜ã€‚")
        else:
            if os.path.exists(DIRS["upload"]):
                existing_pdfs = [f for f in os.listdir(DIRS["upload"]) if f.endswith(".pdf")]
                if existing_pdfs:
                    st.success(f"ğŸ“‚ åœ¨ `1_uploads` ç›®å½•ä¸­æ‰¾åˆ° {len(existing_pdfs)} ä¸ª PDF æ–‡ä»¶ã€‚")
                    
                    # è®©ç”¨æˆ·é€‰æ‹©è¦å¤„ç†å“ªäº› (é»˜è®¤å…¨é€‰)
                    selected_existing = st.multiselect(
                        "é€‰æ‹©è¦å¤„ç†çš„æ–‡ä»¶", 
                        existing_pdfs, 
                        default=existing_pdfs
                    )
                    # æ„é€ å®Œæ•´è·¯å¾„
                    for f in selected_existing:
                        target_files.append(os.path.join(DIRS["upload"], f))
                else:
                    st.warning("âš ï¸ ç›®å½•ä¸ºç©ºï¼Œè¯·å…ˆä¸Šä¼ æ–‡ä»¶ã€‚")
        st.divider()
        col1, col2 = st.columns([1, 1])
        with col1:
            crop_task_type = st.selectbox(
                "é€‰æ‹©è¦æå–çš„æ•°æ®ç±»å‹", 
                list(TASK_DICT.keys()) 
            )
        with col2:
            default_kw = ""
            algo_type = "TOC" 
            if "è‡ªç„¶èµ„æºç¦€èµ‹" in crop_task_type:
                 # default_kw = r"(åœŸåœ°åˆ©ç”¨.*è¡¨|è¡¨.*åœŸåœ°åˆ©ç”¨.*è¡¨)" 
                # default_kw = r"(?s)(?:è¡¨\s*[\d\-\.]*\s*)?åœŸåœ°\s*åˆ©ç”¨.*(?:ç»Ÿè®¡|ç°çŠ¶|)?\s*è¡¨"
                # default_kw = r"(?s)(?:è¡¨\s*[\d\-\.]*\s*)?åœŸ\s*åœ°\s*åˆ©\s*ç”¨.*è¡¨"
                default_kw = r"(?s)(?:è¡¨\s*[\d\-\.]*\s*)?(?:åœŸ\s*åœ°|åœ°\s*ç±»).*(?:åˆ©\s*ç”¨|ç°\s*çŠ¶|ç»Ÿ\s*è®¡).*è¡¨"
                algo_type = "Content"
            elif "å­˜åœ¨é—®é¢˜" in crop_task_type:
                default_kw = "å­˜åœ¨é—®é¢˜"
            elif "æ•´æ²»æ½œåŠ›" in crop_task_type:
                default_kw = "æ•´æ²»å¯è¡Œæ€§åˆ†æ"
            elif "å­é¡¹ç›®" in crop_task_type:
                default_kw = "å­é¡¹ç›®å®‰æ’"
            elif "ç©ºé—´å¸ƒå±€" in crop_task_type:
                default_kw = "ç©ºé—´å¸ƒå±€ä¼˜åŒ–"
            
            keyword = st.text_input("æå–å…³é”®è¯ (æ”¯æŒæ­£åˆ™)", value=default_kw)
            
            if algo_type == "Content" or crop_task_type == "è‡ªå®šä¹‰å…¨æ–‡æœç´¢":
                st.caption("â„¹ï¸ æ¨¡å¼ï¼š**å…¨æ–‡å…³é”®è¯æ‰«æ**")
                use_content_mode = True
            else:
                st.caption("â„¹ï¸ æ¨¡å¼ï¼š**ç›®å½•ç« èŠ‚åŒ¹é…**")
                use_content_mode = False
        
        # --- å¼€å§‹å¤„ç† ---
        error_files = []
        if st.button("å¼€å§‹è‡ªåŠ¨è£å‰ª", type="primary"):
            if not target_files:
                st.error("æ²¡æœ‰å¾…å¤„ç†çš„æ–‡ä»¶ï¼")
            else:
                bar = st.progress(0)
                status = st.empty()
                success_count = 0
                total_files = len(target_files)
                import gc 

                for i, src_path in enumerate(target_files):
                    f_name = os.path.basename(src_path)
                    status.text(f"æ­£åœ¨å¤„ç† ({i+1}/{total_files}): {f_name} ...")
                    
                    try:
                        info = extract_info(f_name)
                        clean_region_name = info["æ–‡ä»¶å"]
                        
                        task_suffix = "data"
                        if crop_task_type in TASK_DICT:
                            task_suffix = TASK_DICT[crop_task_type]
                        else:
                            task_suffix = keyword.replace("*", "")[:5]

                        dst_name = f"{clean_region_name}_{task_suffix}.pdf"
                        dst_path = os.path.join(DIRS["crop"], dst_name)
                        
                        # 3. æ‰§è¡Œè£å‰ª
                        is_ok = False
                        if use_content_mode:
                            is_ok = extract_pages_by_keywords(src_path, dst_path, keyword)
                        else:
                            is_ok = extract_section_to_pdf(src_path, dst_path, keyword)
                        
                        if is_ok: 
                            success_count += 1
                        else:
                            error_files.append(f_name)
                            
                    except Exception as e:
                        print(f"å¤„ç†å‡ºé”™ {f_name}: {e}")
                        error_files.append(f_name)
                    
                    bar.progress((i + 1) / total_files)
                    
                    # æ‰‹åŠ¨æ¸…ç†å†…å­˜
                    gc.collect() 
                if success_count == total_files: 
                    st.success(f"âœ… å…¨éƒ¨å®Œæˆï¼æˆåŠŸ {success_count} ä¸ªã€‚")
                else: 
                    st.warning(f"âš ï¸ å®Œæˆï¼Œä½†æœ‰ {len(error_files)} ä¸ªå¤±è´¥ã€‚å¤±è´¥åˆ—è¡¨ï¼š{error_files}")

    with tab2:
        st.info("å¦‚æœè‡ªåŠ¨è£å‰ªå¤±è´¥ï¼Œå¯åœ¨æ­¤æ‰‹åŠ¨æŒ‡å®šé¡µç ä¿®å¤ã€‚")
        existing_files = [f for f in os.listdir(DIRS["upload"]) if f.endswith(".pdf")]
        
        c1, c2 = st.columns([1, 2])
        with c1:
            # æ‰‹åŠ¨ä¸Šä¼ å•ä¸ªä½œä¸ºè¡¥å……
            manual_file = st.file_uploader("ä¸Šä¼ æ–°æ–‡ä»¶", type=["pdf"], key="manual_uploader")
        with c2:
            # æˆ–è€…ä»å·²æœ‰åˆ—è¡¨é€‰
            sel_file = st.selectbox("æˆ–é€‰æ‹©å·²ä¸Šä¼ çš„æ–‡ä»¶", ["--è¯·é€‰æ‹©--"] + existing_files)
        
        target_file_path = None
        if manual_file:
            target_file_path = os.path.join(DIRS["upload"], manual_file.name)
            with open(target_file_path, "wb") as f: f.write(manual_file.getbuffer())
        elif sel_file != "--è¯·é€‰æ‹©--":
            target_file_path = os.path.join(DIRS["upload"], sel_file)
        
        if target_file_path:
            st.write(f"å½“å‰é€‰ä¸­: `{os.path.basename(target_file_path)}`")
            c_m1, c_m2 = st.columns(2)
            with c_m1:
                manual_task_type = st.selectbox(
                    "è¿™æ˜¯å“ªç±»æ•°æ®ï¼Ÿ", list(TASK_DICT.keys()), key="manual_task_sel"
                )
            with c_m2:
                col_p1, col_p2 = st.columns(2)
                with col_p1: start_p = st.number_input("èµ·å§‹é¡µç ", 1, value=1)
                with col_p2: end_p = st.number_input("ç»“æŸé¡µç ", 1, value=5)
            
            if st.button("âœ‚ï¸ æ‰§è¡Œè£å‰ª", type="primary"):
                f_name = os.path.basename(target_file_path)
                info = extract_info(f_name)
                task_suffix = TASK_DICT[manual_task_type]
                dst_name = f"{info['æ–‡ä»¶å']}_{task_suffix}_manual.pdf"
                dst_path = os.path.join(DIRS["crop"], dst_name)
                
                if extract_section_to_pdf_self(target_file_path, start_p, end_p, dst_path):
                    st.success(f"âœ… ä¿®å¤æˆåŠŸ: {dst_name}")
                    time.sleep(1)
                    st.rerun()
                else:
                    st.error("è£å‰ªå¤±è´¥")

    st.divider()
    st.subheader("ğŸ“‚ ç»“æœæ–‡ä»¶ç®¡ç†")
    cropped_files = []
    if os.path.exists(DIRS["crop"]):
        cropped_files = [f for f in os.listdir(DIRS["crop"]) if f.endswith(".pdf")]
    
    if cropped_files:
        file_data = []
        for f in cropped_files:
            file_path = os.path.join(DIRS["crop"], f)
            file_size_bytes = os.path.getsize(file_path)
            stats = os.stat(file_path)
            if stats.st_size < 1024 * 1024:
                size_str = f"{stats.st_size / 1024:.1f} KB"
            else:
                size_str = f"{stats.st_size / (1024 * 1024):.2f} MB"
            time_str = time.strftime('%Y-%m-%d %H:%M', time.localtime(stats.st_mtime))
            file_data.append({
                "é€‰æ‹©": False,
                "ğŸ“„ æ–‡ä»¶åç§°": f,
                "ğŸ“„ å¤§å°": size_str,
                "ğŸ•’ ä¿®æ”¹æ—¶é—´": time_str,
                "_timestamp": stats.st_mtime 
            })
        file_data.sort(key=lambda x: x["_timestamp"], reverse=True)
        df_display = pd.DataFrame(file_data,index=range(1,len(file_data)+1)).drop(columns=["_timestamp"])

        edited_df = st.data_editor(
            df_display,
            column_config={
                "é€‰æ‹©": st.column_config.CheckboxColumn("é€‰ä¸­", help="å‹¾é€‰è¿›è¡Œæ“ä½œ", width="small"),
                "ğŸ“„ æ–‡ä»¶åç§°": st.column_config.TextColumn(width="large"), # è®©æ–‡ä»¶ååˆ—å®½ä¸€äº›
                "ğŸ“„  æ–‡ä»¶å¤§å°": st.column_config.TextColumn(width="small"),
                "ğŸ•’ ä¿®æ”¹æ—¶é—´": st.column_config.TextColumn(width="medium"),
            },
            hide_index=True,
            width='content', 
            height=300 # å¢åŠ é«˜åº¦ï¼Œé¿å…æ»šåŠ¨æ¡å¤ªçŸ­
        )

        files_to_delete = edited_df[edited_df["é€‰æ‹©"]]["ğŸ“„ æ–‡ä»¶åç§°"].tolist()

        if files_to_delete:
            if st.button(f"ğŸ—‘ï¸ åˆ é™¤é€‰ä¸­çš„ {len(files_to_delete)} ä¸ªæ–‡ä»¶", type="primary"):
                success_num = 0
                for f_del in files_to_delete:
                    try:
                        os.remove(os.path.join(DIRS["crop"], f_del))
                        success_num += 1
                    except Exception as e:
                        st.error(f"åˆ é™¤å¤±è´¥ {f_del}: {e}")
                if success_num > 0:
                    st.success(f"å·²åˆ é™¤ {success_num} ä¸ªæ–‡ä»¶")
                    time.sleep(1)
                    st.rerun()
        
        with st.expander("ğŸ—‘ï¸ ç®¡ç†/åˆ é™¤å·²å¤„ç†æ–‡ä»¶"):
            def delete_callback():
                # ä» Session State è·å–å½“å‰é€‰ä¸­çš„æ–‡ä»¶
                files = st.session_state.get("files_to_delete_key", [])
                if not files:
                    return # æ²¡é€‰æ–‡ä»¶ï¼Œç›´æ¥è¿”å›
                success_num = 0
                fail_num = 0
                
                for f_del in files:
                    path_to_del = os.path.join(DIRS["crop"], f_del)
                    try:
                        if os.path.exists(path_to_del):
                            os.remove(path_to_del)
                            success_num += 1
                    except:
                        fail_num += 1
                
                st.session_state["delete_result_msg"] = (success_num, fail_num)
                
                st.session_state["files_to_delete_key"] = []

            c_btn1, c_btn2, c_space = st.columns([1, 1, 4])
            
            if c_btn1.button("âœ… å…¨é€‰"):
                st.session_state["files_to_delete_key"] = cropped_files
                st.rerun()        
            if c_btn2.button("â¬œ æ¸…ç©º"):
                st.session_state["files_to_delete_key"] = []
                st.rerun()
            st.multiselect(
                "é€‰æ‹©è¦åˆ é™¤çš„æ–‡ä»¶ (æ”¯æŒå¤šé€‰)", 
                cropped_files,
                key="files_to_delete_key" 
            )

            # --- 3. åˆ é™¤æŒ‰é’® (ç»‘å®šå›è°ƒ) ---
            st.button("ğŸš¨ ç¡®è®¤åˆ é™¤é€‰ä¸­æ–‡ä»¶", type="primary", on_click=delete_callback)

            # --- 4. æ˜¾ç¤ºæ“ä½œç»“æœ ---
            if "delete_result_msg" in st.session_state:
                s_count, f_count = st.session_state["delete_result_msg"]
                
                if s_count > 0:
                    st.success(f"âœ… å·²æˆåŠŸåˆ é™¤ {s_count} ä¸ªæ–‡ä»¶ï¼")
                if f_count > 0:
                    st.warning(f"âš ï¸ {f_count} ä¸ªæ–‡ä»¶åˆ é™¤å¤±è´¥ã€‚")
                
                del st.session_state["delete_result_msg"]

        col_d1, col_d2 = st.columns(2)
        with col_d1:
            st.subheader("ğŸ“¦ æ–‡ä»¶ä¸‹è½½")
            source_dir = DIRS["crop"]
            all_pdfs = []
            if os.path.exists(source_dir):
                all_pdfs = [f for f in os.listdir(source_dir) if f.endswith(".pdf")]

            if not all_pdfs:
                st.info("æš‚æ— æ–‡ä»¶å¯ä¸‹è½½")
            else:
                # 2. åˆ›å»ºç­›é€‰é€‰é¡¹ï¼š ["æ‰€æœ‰æ–‡ä»¶"] + [TASK_DICT çš„ä¸­æ–‡é”®å]
                download_options = ["æ‰€æœ‰æ–‡ä»¶"] + list(TASK_DICT.keys())
                
                # è®©ç”¨æˆ·é€‰æ‹©ä¸‹è½½ç±»å‹
                selected_type = st.selectbox(
                    "é€‰æ‹©è¦ä¸‹è½½çš„æ•°æ®ç±»å‹", 
                    download_options, 
                    key="download_type_selector"
                )

                # 3. æ ¹æ®é€‰æ‹©è¿›è¡Œæ–‡ä»¶ç­›é€‰
                files_to_zip = []
                zip_filename = "download.zip"

                if selected_type == "æ‰€æœ‰æ–‡ä»¶":
                    files_to_zip = all_pdfs
                    zip_filename = "all_cropped_files.zip"
                else:
                    # è·å–å¯¹åº”çš„è‹±æ–‡åç¼€ï¼Œä¾‹å¦‚ "landuse"
                    suffix = TASK_DICT[selected_type]
                    # ç­›é€‰ç»“å°¾åŒ¹é… _{suffix}.pdf çš„æ–‡ä»¶
                    # æ³¨æ„ï¼šæˆ‘ä»¬è¦åŒ¹é…å¦‚ "xxx_landuse.pdf"
                    target_ending = f"_{suffix}.pdf"
                    
                    files_to_zip = [f for f in all_pdfs if f.endswith(target_ending)]
                    zip_filename = f"{suffix}_files.zip"

                # 4. ç”Ÿæˆå¹¶æ˜¾ç¤ºä¸‹è½½æŒ‰é’®
                if files_to_zip:
                    zip_buffer = io.BytesIO()
                    with zipfile.ZipFile(zip_buffer, "w", zipfile.ZIP_DEFLATED) as zf:
                        for f_name in files_to_zip:
                            file_full_path = os.path.join(source_dir, f_name)
                            zf.write(file_full_path, arcname=f_name)
                    
                    # å°†æŒ‡é’ˆç§»å›å¤´éƒ¨
                    zip_buffer.seek(0)

                    # æ˜¾ç¤ºä¸‹è½½æŒ‰é’®
                    st.download_button(
                        label=f"ğŸ“¥ ä¸‹è½½ {selected_type} ({len(files_to_zip)}ä¸ª)",
                        data=zip_buffer,
                        file_name=zip_filename,
                        mime="application/zip",
                        type="primary"
                    )
                else:
                    st.warning(f"æœªæ‰¾åˆ°å±äºâ€œ{selected_type}â€ç±»å‹çš„æ–‡ä»¶ (éœ€åŒ…å« _{TASK_DICT.get(selected_type)} åç¼€)")
# ========================================================
# 2. æ•°æ®æå– (API)
# ========================================================
elif step == "2. å¤§æ¨¡å‹æ•°æ®è·å–":
    st.header("ğŸ¤– æ­¥éª¤ 2: è°ƒç”¨ AI æå–æ•°æ®")
    col1, col2 = st.columns([1, 1])
    with col1:
        task_type = st.selectbox("é€‰æ‹©åˆ†æä»»åŠ¡ç±»å‹", list(TASK_DICT.keys()))
    
    target_suffix = TASK_DICT.get(task_type)
    
    # 2. scan crop directory for relevant files
    if not os.path.exists(DIRS["crop"]):
        st.warning("âš ï¸ è£å‰ªç›®å½•ä¸å­˜åœ¨ã€‚")
    else:
        all_pdfs = [f for f in os.listdir(DIRS["crop"]) if f.endswith(".pdf")]
        
        target_files = [f for f in all_pdfs if f.endswith(f"_{target_suffix}.pdf")]
        
        if not target_files:
            st.warning(f"âš ï¸ æœªæ‰¾åˆ°åç¼€ä¸º `_{target_suffix}.pdf` çš„æ–‡ä»¶ã€‚")
            st.info("è¯·å›åˆ° **æ­¥éª¤ 1**ï¼Œé€‰æ‹©å¯¹åº”çš„æ•°æ®ç±»å‹å¹¶æ‰§è¡Œè£å‰ªã€‚")
        else:
            st.subheader(f"1ï¸âƒ£ å¾…å¤„ç†æ–‡ä»¶åˆ—è¡¨ ({len(target_files)} ä¸ª)")
            # preview file info
            file_info_list = []
            for f in target_files:
                info = extract_info(f)
                file_info_list.append(info)
            
            st.dataframe(
                pd.DataFrame(file_info_list,index=range(1,len(file_info_list)+1))[["åŸå§‹æ–‡ä»¶å", "æ–‡ä»¶å", "åŸå¸‚", "åœ°åŒº/å¿"]], 
                height=150,
                width="stretch"
            )
            
            st.divider()
            st.subheader("2ï¸âƒ£ å¼€å§‹æå–")
            
            if st.button("ğŸš€ å¤§æ¨¡å‹è§£æï¼Œæ•°æ®æå–", type="primary"):
                results = []
                progress_bar = st.progress(0)
                log_container = st.container()
                
                client = None
                workflow_id = None

                client = CozeClient()
                workflow_id = WORKFLOW_CONFIG.get(task_type) # ç›´æ¥ç”¨å®Œæ•´keyæˆ–ç®€å•keyï¼Œå–å†³äºapi_clienté…ç½®
                # åªéå†ç­›é€‰åçš„æ–‡ä»¶
                for i, info in enumerate(file_info_list):
                    file_name = info["åŸå§‹æ–‡ä»¶å"]
                    file_path = os.path.join(DIRS["crop"], file_name)
                    region_name = info["æ–‡ä»¶å"] 
                    with log_container:
                        status_expander = st.expander(f"ğŸ”„ æ­£åœ¨å¤„ç†: {region_name} ...", expanded=True)
                        with status_expander:
                            st.write(f"ğŸ“„ æ–‡ä»¶: `{file_name}`")
                            raw_data = None
                            try:
                                    if not workflow_id:
                                        st.error(f"âŒ æœªé…ç½® '{task_type}' çš„ Workflow ID")
                                    else:
                                        st.write("ğŸ“¤ ä¸Šä¼ ä¸­...")
                                        file_id = client.upload_file(file_path)
                                        if file_id:
                                            st.write("ğŸ¤– åˆ†æä¸­...")
                                            raw_data = client.run_workflow(workflow_id, file_id)
                                            if raw_data: st.success("âœ… æˆåŠŸ")
                                            else: st.error("âŒ è¿”å›ä¸ºç©º")
                                        else: st.error("âŒ ä¸Šä¼ å¤±è´¥")
                                        time.sleep(1)
                            except Exception as e:
                                st.error(f"âŒ å¼‚å¸¸: {e}")
                            
                            if raw_data:
                                try:
                                    json_data = json.loads(raw_data)
                                    if "output" in json_data:
                                        st.text_area("Output æ–‡æœ¬", json_data["output"], height=200)
                                except: pass
                                results.append({
                                    "åœ°åŒº": region_name, 
                                    "rawdata": raw_data, 
                                    "åŸå§‹æ–‡ä»¶å": file_name
                                })
                    
                    progress_bar.progress((i + 1) / len(target_files))
                
                st.success(f"ğŸ‰ å¤„ç†å®Œæˆï¼è·å– {len(results)} æ¡æ•°æ®ã€‚")
                
                if results:
                    df_result = pd.DataFrame(results)
                    # ä¿å­˜æ–‡ä»¶åå¸¦ä¸Šåç¼€ï¼Œå¯¹åº” Step 3 çš„è¯»å–
                    save_filename = f"coze_raw_output_{target_suffix}.csv"
                    save_path = os.path.join(DIRS["raw"], save_filename)
                    
                    df_result.to_csv(save_path, index=False, encoding='utf-8-sig')
                    st.write(f"æ•°æ®å·²åˆ†ç±»ä¿å­˜è‡³: `{save_path}`")
                    st.dataframe(df_result.head())
            
    # æ–‡ä»¶ç®¡ç†
    render_file_manager(DIRS["raw"], title="å¤§æ¨¡å‹è·å–çš„æ•°æ®", file_ext=".csv", key_prefix="step2")
    
# # ========================================================
# # 3. æ•°æ®è§£æ
# # ========================================================
elif step == "3. æ•°æ®è§£æ":
    st.header("ğŸ§¹ æ­¥éª¤ 3: ç»“æ„åŒ–æ•°æ®è§£æ")
    tab1, tab2,tab3 = st.tabs(["âš™ï¸ è§£æåŸå§‹æ•°æ®", 
                               "ğŸ“¤ ä¸Šä¼ å¤–éƒ¨æ•°æ® (è¡¥å……ç¼ºå¤±é¡¹)",
                               "ğŸ”„ åŠ è½½å†å²ä¸­é—´æ•°æ® (.pkl)"])
    
    # æ­£å¸¸æ•°æ®è§£æ
    with tab1:
        col1, col2 = st.columns([1, 1])
        with col1:
            parse_type = st.selectbox("é€‰æ‹©è§£ææ•°æ®ç±»å‹", list(TASK_DICT.keys()))
        task_suffix = TASK_DICT[parse_type]
        raw_filename = f"coze_raw_output_{task_suffix}.csv"
        raw_file = os.path.join(DIRS["raw"], raw_filename)
        
        if not os.path.exists(raw_file):
            st.warning(f"âš ï¸ æœªæ‰¾åˆ°å¯¹åº”çš„æ•°æ®æ–‡ä»¶ï¼š{raw_filename}ã€‚è¯·å…ˆå®Œæˆæ­¥éª¤ 2 ä¸­è¯¥ç±»å‹çš„æå–ã€‚")
        else:
            df_raw = pd.read_csv(raw_file)
            df_raw_preview = df_raw.head()
            df_raw_preview.index = df_raw_preview.index + 1
            st.write(f"ğŸ“‚ è¯»å–æ•°æ®æº: `{raw_filename}`")
            st.write("åŸå§‹æ•°æ®é¢„è§ˆ:", df_raw_preview.head(3))
            
            if col2.button("æ•°æ®è§£æ", type="primary"):
                # 1. è°ƒç”¨ utils_parsers ä¸­çš„å¤„ç†å‡½æ•°
                # process_raw_data ä¼šè¿”å›çº¯ç‰¹å¾æ•°æ®çš„ DataFrame (ä¸å«åœ°åŒºåˆ—)
                parsed_df = process_raw_data(df_raw, parse_type)
                
                # 2. åˆå¹¶åœ°åŒºåˆ— (ç¡®ä¿æ•°æ®å¯¹é½)
                # å…³é”®ï¼šç¡®ä¿ parsed_df çš„ç´¢å¼•ä¸ df_raw ä¸€è‡´ï¼Œé˜²æ­¢é”™ä½
                parsed_df.index = df_raw.index 
                
                # ä½¿ç”¨ join æˆ–è€… concat (axis=1)
                # åªå– 'åœ°åŒº' åˆ—å’Œæ–°ç”Ÿæˆçš„ç‰¹å¾åˆ—
                final_df = pd.concat([df_raw[['åœ°åŒº']], parsed_df], axis=1)
                
                # 3. æ„é€ è¾“å‡ºæ–‡ä»¶å (parsed_landuse.csv, parsed_issue.csv ...)
                out_name = f"parsed_{task_suffix}.csv"
                save_path = os.path.join(DIRS["result"], out_name)
                
                # 4. ä¿å­˜
                final_df.to_csv(save_path, index=False, encoding='utf-8-sig')
                
                st.success(f"âœ… è§£ææˆåŠŸï¼ç»“æœå·²ä¿å­˜è‡³: {out_name}")
                st.dataframe(final_df.head())
    # æ‰‹åŠ¨ä¸Šä¼ å¤–éƒ¨æ•°æ®
    with tab2:
        st.markdown("""
        **åŠŸèƒ½è¯´æ˜ï¼š** å¦‚æœæŸäº›æ•°æ®ï¼ˆå¦‚ç©ºé—´å¸ƒå±€è§„åˆ’ï¼‰æ— æ³•é€šè¿‡ `PDF` æå–ï¼Œæˆ–è€…æ‚¨å·²ç»æœ‰æ•´ç†å¥½çš„ `Excel/CSV `æ•°æ®ï¼Œåœ¨æ­¤å¤„ä¸Šä¼ ã€‚
        ç³»ç»Ÿä¼šè‡ªåŠ¨å°†å…¶ä¿å­˜ä¸ºæ ‡å‡†æ ¼å¼ï¼Œä»¥ä¾¿åç»­æ­¥éª¤è¿›è¡Œèåˆã€‚
        """)
        c1, c2 = st.columns([1, 1])
        with c1:
            upload_type = st.selectbox("é€‰æ‹©ä¸Šä¼ çš„æ•°æ®ç±»å‹", list(TASK_DICT.keys()), key="upload_type_sel")
            target_suffix = TASK_DICT[upload_type]
        with c2:
            # ç”Ÿæˆæ¨¡æ¿ä¸‹è½½
            st.write("ğŸ“ **æ•°æ®æ ¼å¼è¦æ±‚ï¼š**")
            st.caption("å¿…é¡»åŒ…å« `åœ°åŒº` åˆ—ï¼Œå…¶ä»–åˆ—ä¸ºç‰¹å¾æ•°å€¼ã€‚")
            # è·å–å¯¹åº”çš„æ¨¡æ¿åˆ—å
            cols = TEMPLATE_COLUMNS.get(target_suffix, TEMPLATE_COLUMNS["default"])
            template_df = pd.DataFrame(columns=cols)
            template_csv = template_df.to_csv(index=False).encode('utf-8-sig')
            st.download_button(f"ğŸ“¥ ä¸‹è½½ {upload_type} æ¨¡æ¿", template_csv, f"template_{target_suffix}.csv", "text/csv")

        uploaded_ext = st.file_uploader("ä¸Šä¼ å¤„ç†å¥½çš„æ–‡ä»¶ (.csv / .xlsx)", type=["csv", "xlsx"])
        
        if uploaded_ext:
            try:
                if uploaded_ext.name.endswith('.csv'):
                    ext_df = pd.read_csv(uploaded_ext)
                else:
                    ext_df = pd.read_excel(uploaded_ext)
                if "åœ°åŒº" not in ext_df.columns:
                    st.error("âŒ ä¸Šä¼ å¤±è´¥ï¼šæ–‡ä»¶ä¸­ç¼ºå°‘ `åœ°åŒº` åˆ—ï¼è¯·å‚ç…§æ¨¡æ¿æ ¼å¼ã€‚")
                    st.write("å½“å‰åˆ—å:", list(ext_df.columns))
                else:
                    # é¢„è§ˆ
                    st.write("ğŸ“Š æ•°æ®é¢„è§ˆ:", ext_df.head())
                    if st.button("ğŸ’¾ ç¡®è®¤å¹¶ä¿å­˜"):
                        target_name = f"parsed_{target_suffix}_manual.csv"
                        save_path = os.path.join(DIRS["result"], target_name)
                        ext_df.to_csv(save_path, index=False, encoding='utf-8-sig')
                        
                        st.success(f"âœ… æ–‡ä»¶å·²ä¿å­˜ä¸º: `{target_name}`")
                        st.info("ğŸ’¡ ç°åœ¨æ‚¨å¯ä»¥å‰å¾€ **æ­¥éª¤ 4**ï¼Œè¯¥æ–‡ä»¶å°†è‡ªåŠ¨å‚ä¸æ•°æ®èåˆã€‚")    
            except Exception as e:
                st.error(f"æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
    with tab3:
        st.markdown("""
        **åŠŸèƒ½è¯´æ˜ï¼š** å¦‚æœæ‚¨ä¹‹å‰ä¿å­˜äº†å¤„ç†è¿‡ç¨‹ä¸­çš„ `.pkl` (Pickle) æ–‡ä»¶ï¼Œå¯ä»¥ç›´æ¥åœ¨æ­¤å¤„æ¢å¤ã€‚
        ç³»ç»Ÿä¼šè‡ªåŠ¨å°†å…¶è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼ï¼Œ**æ‚¨å¯ä»¥ç›´æ¥è·³è¿‡è§£ææ­¥éª¤ï¼Œç›´æ¥è¿›è¡Œæ­¥éª¤ 4 çš„èåˆä¸æ­¥éª¤ 5 çš„åˆ†ç±»**ã€‚
        """)
        uploaded_pkl = st.file_uploader("ä¸Šä¼ å¤„ç†å¥½çš„ .pkl å­—å…¸æ–‡ä»¶", type=["pkl"], key="tab3_uploader")
        
        if uploaded_pkl:
            try:
                data_dict = pd.read_pickle(uploaded_pkl)
                required_keys = {'X', 'features', 'regions'}
                if isinstance(data_dict, dict) and required_keys.issubset(data_dict.keys()):
                    st.success("âœ… æ£€æµ‹åˆ°åˆæ³•çš„ç‰¹å¾å­—å…¸ç»“æ„ï¼")
                    regions = data_dict['regions']
                    feats = data_dict['features']
                    
                    st.write(f"ğŸ“Š æ•°æ®ç»´åº¦: {len(regions)} ä¸ªåœ°åŒº Ã— {len(feats)} ä¸ªç‰¹å¾")
                    # æ­¤å¤„éœ€è¦è¿›è¡Œæ›´æ”¹éœ€é‚£ç§ norm
                    # æ—¢ç„¶å·²ç»æœ‰ X_normï¼Œæˆ‘ä»¬å…è®¸ç”¨æˆ·é€‰æ‹©æ˜¯å¦ç›´æ¥ä½¿ç”¨å®ƒ
                    use_norm_data = st.checkbox("ä½¿ç”¨å·²å½’ä¸€åŒ–çš„æ•°æ® (X_norm)", value=True, 
                                              help="å¦‚æœé€‰ä¸­ï¼Œå°†ä½¿ç”¨ pkl ä¸­çš„ X_norm ç›´æ¥ç”Ÿæˆæœ€ç»ˆçŸ©é˜µï¼›å¦åˆ™ä½¿ç”¨ X é‡æ–°ç”Ÿæˆã€‚")
                    matrix_data = preprocess_X(data_dict['X'], eps=1e-8, use_log=True) if use_norm_data  else data_dict['X']
                    if len(regions) == matrix_data.shape[0] and len(feats) == matrix_data.shape[1]:
                        df_reconstructed = pd.DataFrame(matrix_data, index=regions, columns=feats)
                        df_reconstructed.index.name = "åœ°åŒº"
                        
                        st.dataframe(df_reconstructed.head(3))
                        col_btn1, col_btn2 = st.columns([1,2])
                        with col_btn1:
                            if st.button("ğŸš€ æ¢å¤ä¸ºæœ€ç»ˆçŸ©é˜µ", type="primary"):
                                # === æ ¸å¿ƒæ“ä½œï¼šç›´æ¥ç”Ÿæˆ Step 4 çš„äº§å‡ºæ–‡ä»¶ ===
                                # ä¿å­˜ä¸º parsed_final_matrix.csvï¼Œè¿™æ · Step 5 å¯ä»¥ç›´æ¥è¯»å–
                                histroy_files = "parsed_origion_matrix.csv"
                                history_x_files = "parsed_hist_matrix.csv"
                                save_path_final = os.path.join(DIRS["result"], histroy_files)
                                df_reconstructed.to_csv(save_path_final, encoding='utf-8-sig')
                                if 'X' in data_dict:
                                    df_raw_backup = pd.DataFrame(data_dict['X'], index=regions, columns=feats)
                                    df_raw_backup.index.name = "åœ°åŒº"
                                    df_raw_backup.to_csv(os.path.join(DIRS["result"],history_x_files ), encoding='utf-8-sig')
                                st.success(f"âœ… æ•°æ®å·²æ¢å¤ï¼")
                                st.info("ğŸ’¡ æ‚¨ç°åœ¨å¯ä»¥ç›´æ¥ç‚¹å‡»ä¾§è¾¹æ çš„ **'5. æ•°æ®åˆ†ç±»ä¸å¯¼å‡º'** è¿›è¡Œåˆ†æã€‚")
                    else:
                        st.error(f"âŒ ç»´åº¦ä¸åŒ¹é…ï¼šåœ°åŒºæ•° {len(regions)} vs çŸ©é˜µè¡Œæ•° {matrix_data.shape[0]}") 
                else:
                    st.error(f"âŒ æœªçŸ¥çš„æ•°æ®ç»“æ„ã€‚Keys: {data_dict.keys() if isinstance(data_dict, dict) else type(data_dict)}")

            except Exception as e:
                st.error(f"âŒ è¯»å–å‡ºé”™: {e}")
    render_file_manager(DIRS["result"], title="å·²è§£æçš„ç»“æ„åŒ–æ•°æ®", file_ext=".csv", key_prefix="step3")
# # ========================================================
# # 4. æ•°æ®èåˆ
# # ========================================================
elif step == "4. æ•°æ®èåˆ&å±•ç¤º":
    st.header("ğŸ”— æ­¥éª¤ 4: å¤šæºæ•°æ®èåˆåŠå¯è§†åŒ–å±•ç¤º")
    # scan parser CSV files
    csvs = [f for f in os.listdir(DIRS["result"]) if not f.startswith("fusion")]
    norm_res_path = ""
    raw_res_path = ""
    if not csvs:
        st.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°è§£æåçš„æ•°æ®æ–‡ä»¶ï¼Œè¯·å…ˆå®Œæˆæ­¥éª¤ 3ã€‚")
    else:
        st.info("ğŸ’¡ æç¤ºï¼šä¸ºäº†ä¿è¯å½’ä¸€åŒ–ç´¢å¼•æ­£ç¡®ï¼Œç³»ç»Ÿå°†æŒ‰ç…§ **[è‡ªç„¶èµ„æº -> æ½œåŠ› -> ç©ºé—´ -> é—®é¢˜ -> é¡¹ç›®]** çš„é¡ºåºå¼ºåˆ¶æ’åºã€‚")
        # === æ ¸å¿ƒé€»è¾‘ï¼šå¼ºåˆ¶æ–‡ä»¶æ’åº ===
        # å®šä¹‰æœŸæœ›çš„å…³é”®è¯é¡ºåºï¼ˆä¸ preprocess_X ä¸­çš„ç¡¬ç¼–ç ç´¢å¼•å¯¹åº”ï¼‰
        # 1.è‡ªç„¶èµ„æº: 0-3
        # 2.æ½œåŠ›: 4-22
        # 3.ç©ºé—´: 23-27
        # 4.é—®é¢˜: 28-32
        # 5.é¡¹ç›®: 33+
        # 1. å®šä¹‰æ ¸å¿ƒä»»åŠ¡åç¼€é¡ºåº
        strict_order_suffixes = ["LandUse", "potential", "spatial", "issue", "project"]
        history_filename = "parsed_hist_matrix.csv"
        
        # 2. Default Selection - only include those that exist
        default_files = []
        for suffix in strict_order_suffixes:
            target_name = f"parsed_{suffix}.csv"
            if target_name in csvs:
                default_files.append(target_name)
        has_history = history_filename in csvs
        # core files|fusion hist files
        col_sel1, col_sel2 = st.columns([2, 1])
        with col_sel1:
            selected_core = st.multiselect(  
            "é€‰æ‹©è¦èåˆçš„æ–‡ä»¶ (é»˜è®¤ä»…é€‰ä¸­ 5 ç±»æ ¸å¿ƒæ•°æ®)", 
            options=[f for f in csvs if f != history_filename], # æ’é™¤å†å²æ–‡ä»¶ï¼Œé˜²æ­¢æ··æ·†
            default=default_files,
            help="é€‰æ‹©æ‚¨åˆšåˆšä» PDF è§£æå‡ºæ¥çš„ parsed_*.csv æ–‡ä»¶ã€‚"
            )
        use_history = False
        with col_sel2:
            st.write("ğŸ“š **å†å²æ•°æ®èåˆ**")
            use_history = False
            if has_history:
                use_history = st.checkbox(
                    "â• èåˆå†å²æ¢å¤æ•°æ®", 
                    value=True, 
                    help=f"æ£€æµ‹åˆ° Step 3 æ¢å¤äº† `{history_filename}`ï¼Œå‹¾é€‰æ­¤é¡¹å°†å…¶ä¸æ–°æ•°æ®åˆå¹¶ã€‚"
                )
                if use_history:
                    st.caption(f"âœ… å·²åŒ…å«: `{history_filename}`")
            else:
                st.caption("ğŸš« æœªæ£€æµ‹åˆ°å†å²æ¢å¤æ•°æ® (pkl)")
        # 4. æ„å»ºæœ€ç»ˆå¾…èåˆåˆ—è¡¨
        final_selected_files = default_files.copy()
        if not default_files:
            st.warning("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•ç¬¦åˆæ ‡å‡†å‘½åè§„èŒƒçš„æ ¸å¿ƒæ–‡ä»¶ï¼ˆå¦‚ parsed_landuse.csvï¼‰ã€‚è¯·æ£€æŸ¥æ­¥éª¤ 3 æ˜¯å¦å·²æ­£ç¡®æ‰§è¡Œã€‚")
        c1, c2 = st.columns([1, 2])
        with c1:
            use_log = st.checkbox("â˜‘ï¸ å¯ç”¨å¯¹æ•°å˜æ¢", value=True, help="å¯¹é¢ç§¯/é‡‘é¢/æ•°é‡åˆ—è¿›è¡Œ Log(x+1) å˜æ¢ï¼Œæ‹‰è¿‘é•¿å°¾åˆ†å¸ƒçš„å·®è·ï¼Œé¿å…å°æ•°å€¼åœ¨å½’ä¸€åŒ–åå˜ä¸º0ã€‚")
        with c2:
            start_btn = st.button("å¼€å§‹èåˆä¸å½’ä¸€åŒ–", type="primary")
        
        # output paths
        suffix = "_log" if use_log else ""
        norm_filename = f"fusion_final_matrix{suffix}.csv"
        raw_filename = f"fusion_raw_matrix.csv"
        
        norm_res_path = os.path.join(DIRS["result"], norm_filename)
        raw_res_path = os.path.join(DIRS["result"], raw_filename)
            
        if  start_btn:
            if not final_selected_files:
                st.error("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªæ–‡ä»¶ã€‚")
            else:
                matrices, maps, names,all_feature_names = [], [], [],[]
                sorted_files = []   
                # process file combine
                for f in final_selected_files:
                    path = os.path.join(DIRS["result"], f)
                    df = pd.read_csv(path)
                    
                    region_col = df.columns[0]
                    df = df.set_index(region_col)
                    df_num = df.select_dtypes(include=['number']).fillna(0)
                    
                    matrices.append(df_num.values)
                    maps.append({name: i for i, name in enumerate(df_num.index)})
                    
                    feat_prefix = f.replace("parsed_", "").replace(".csv", "")
                    names.append(feat_prefix)
                    all_feature_names.extend([f"{feat_prefix}:{c}" for c in df_num.columns])

                regions, X_final, slices = unify_and_concatenate(matrices, maps, names)
                history_msg = ""
                # process history data
                if use_history:
                    hist_path = os.path.join(DIRS["result"], history_filename)
                    df_hist = pd.read_csv(hist_path)
                    region_his = df_hist["åœ°åŒº"].tolist()
                    df_hist = df_hist.set_index("åœ°åŒº")
                    
                    # regfion2index
                    df_hist_aligned = pd.DataFrame(index=df_hist.index)
                    maps = {name: i for i, name in enumerate(region_his)} 
                    # feature alignment
                    for target_col in all_feature_names:
                        raw_col_name = target_col.split(":")[-1] 
                        if raw_col_name in df_hist.columns:
                            df_hist_aligned[raw_col_name] = df_hist[raw_col_name]
                        elif raw_col_name.endswith('æ’åº'):
                            issue_feature = raw_col_name.split('_')[0] 
                            df_hist_aligned[raw_col_name] = df_hist['å­˜åœ¨_'+issue_feature]
                    df_hist_num = df_hist_aligned.fillna(0)
                    hist_matrix = df_hist_num.values
                    current_regions_set = set(region_his)
                    unique_indices = [i for i, r in enumerate(current_regions_set) if r not in regions]
                    if unique_indices:
                        unique_regions = [region_his[i] for i in unique_indices]
                        unique_matrix = hist_matrix[unique_indices] 
                        regions = regions + unique_regions
                        X_final = np.vstack([X_final, unique_matrix])
                        history_msg = f" (å·²åŒ…å« {len(unique_regions)} ä¸ªå†å²è¡¥å…¨åœ°åŒº)"
                    else:
                        st.caption("â„¹ï¸ å†å²æ•°æ®åœ°åŒºå·²å…¨éƒ¨å­˜åœ¨ï¼Œæœªæ–°å¢è¡Œã€‚")
                if len(regions) > 0:
                    st.success(f"âœ… èåˆæˆåŠŸï¼å…± {len(regions)} ä¸ªåœ°åŒº{history_msg}ï¼Œç‰¹å¾ç»´åº¦: {X_final.shape[1]}")
                    try:
                        raw_df = pd.DataFrame(X_final, index=regions, columns=all_feature_names)
                        raw_df.index.name = "åœ°åŒº"
                        raw_df.to_csv(raw_res_path, encoding='utf-8-sig', index_label="åœ°åŒº")
                        
                        st.info(f"æ­£åœ¨å¤„ç†... (Logå˜æ¢: {use_log})")
                        X_norm = preprocess_X(X_final, use_log=use_log)
                        
                        final_df = pd.DataFrame(X_norm, index=regions, columns=all_feature_names)
                        final_df.index.name = "åœ°åŒº"
                        final_df.to_csv(norm_res_path, encoding='utf-8-sig', index_label="åœ°åŒº")
                        time.sleep(5)
                        st.rerun()
                    except Exception as e:
                            st.error(f"å½’ä¸€åŒ–å¤±è´¥: {e}")
                else:
                        st.error("èåˆå¤±è´¥ï¼šæ‰€é€‰æ•°æ®è¡¨ä¹‹é—´æ²¡æœ‰å…¬å…±åœ°åŒºã€‚")                
    if os.path.exists(norm_res_path):
        st.divider()
        st.subheader("ğŸ¨ å¤šç»´åº¦å¯è§†åŒ–")
        # 1. å‡†å¤‡é€‰é¡¹ï¼šè‡ªåŠ¨æ‰«æ result ç›®å½•
        vis_options = {}
        # æ‰¾æœ€ç»ˆçŸ©é˜µ (æ ¹æ®ä½ çš„æ–‡ä»¶åç‰¹å¾)
        final_files = [f for f in os.listdir(DIRS["result"]) if "fusion_final" in f]
        for f in final_files:
            vis_options[f"ğŸ† æœ€ç»ˆèåˆçŸ©é˜µ ({f})"] = os.path.join(DIRS["result"], f)
        # æ‰¾å…¶ä»–åˆ†é¡¹æ•°æ®
        sub_files = [f for f in os.listdir(DIRS["result"]) if "fusion_final" not in f and f.endswith(".csv")]
        no_vis_files = ['parsed_issue.csv','parsed_ptential.csv','parsed_potential.csv',]  # è¿™äº›æ–‡ä»¶ä¸é€‚åˆå¯è§†åŒ–
        vis_files = [f for f in sub_files if f not in no_vis_files ]
        for f in vis_files:
                vis_options[f"ğŸ“„ åˆ†é¡¹æ•°æ®: {f}"] = os.path.join(DIRS["result"], f)

        # 2. ç”¨æˆ·é€‰æ‹©
        c_vis1, c_vis2 = st.columns([2, 1])
        with c_vis1:
            selected_vis_key = st.selectbox("é€‰æ‹©è¦å±•ç¤ºçš„æ•°æ®:", list(vis_options.keys()))
        
        target_path = vis_options[selected_vis_key]
        
        try:
            df_vis = pd.read_csv(target_path, index_col=0)
            # ä»…ä¿ç•™æ•°å€¼åˆ—
            df_vis = df_vis.select_dtypes(include=['number'])

            if df_vis.empty:
                st.warning("æ•°æ®ä¸ºç©ºï¼Œæ— æ³•ç»˜å›¾")
            else:
                # === æ ¸å¿ƒåˆ¤æ–­é€»è¾‘ ===
                is_final_result = "æœ€ç»ˆèåˆ" in selected_vis_key

                if is_final_result:
                    # Case A: æœ€ç»ˆç»“æœ -> ç›´æ¥è¯»å–ï¼ŒåŸæ ·ç»˜åˆ¶
                    # ä½ çš„é¢„å¤„ç†å·²ç»ä¿è¯äº†å®ƒåœ¨ 0-1 ä¹‹é—´ä¸”æ²¡æœ‰ 0 å€¼
                    with c_vis2:
                        st.success("âœ… æ£€æµ‹åˆ°é¢„å¤„ç†åçš„èåˆçŸ©é˜µï¼Œå·²ç›´æ¥å±•ç¤ºã€‚")
                        # è¿™é‡Œä¸éœ€è¦ä»»ä½• Checkbox
                    
                    # ç›´æ¥ç”»å›¾ (df_vis å·²ç»æ˜¯å®Œç¾çŠ¶æ€)
                    fig = plot_heatmap(df_vis.values, df_vis.index.tolist(), feature_names=df_vis.columns.tolist())
                    history_tag = "_with_history" if use_history else ""
                    fig.savefig(f"{DIRS['result']}/final_result_heatmap{suffix}{history_tag}.png", dpi=300, bbox_inches='tight')
                    st.pyplot(fig)
                else:
                    with c_vis2:
                        # 1. Log
                        df_proc = np.log1p(np.maximum(df_vis, 0))
                        # 2. Min-Max
                        range_val = df_proc.max() - df_proc.min()
                        df_plot = df_proc.copy()
                        for col in df_proc.columns:
                            if range_val[col] > 1e-8:
                                df_plot[col] = (df_proc[col] - df_proc[col].min()) / range_val[col]
                            else:
                                df_plot[col] = 0
                        name = selected_vis_key.split(":")[-1].split('.')[0]
                        history_tag = "_with_history" if use_history else ""
                        fig = plot_heatmap(df_plot.values, df_plot.index.tolist(), feature_names=df_plot.columns.tolist())
                        fig.savefig(f"{DIRS['result']}/norm_{name}_heatmap{suffix}{history_tag}.png", dpi=300, bbox_inches='tight')
                    st.pyplot(fig)

        except Exception as e:
            st.error(f"ç»˜å›¾å‡ºé”™: {e}")
    
    # è¿™é‡Œå±•ç¤ºçš„æ˜¯ result ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶ï¼ˆåŒ…å« Step 3 çš„è§£ææ–‡ä»¶å’Œ Step 4 çš„çŸ©é˜µæ–‡ä»¶ï¼‰               
    render_file_manager(DIRS["result"], title="èåˆåŠä¸­é—´æ•°æ®ç®¡ç†", file_ext=".csv", key_prefix="step4")
# ========================================================
# 5. æ•°æ®åˆ†ç±»ä¸å¯¼å‡º
# ========================================================
elif step == "5. æ•°æ®åˆ†ç±»ä¸å¯¼å‡º":
    st.header("ğŸ“Š æ­¥éª¤ 5: æ™ºèƒ½åˆ†åŒºåˆ†ç±»")
    df_matrix = None
    data_version = st.radio(
            "é€‰æ‹©è¦ä½¿ç”¨çš„çŸ©é˜µç‰ˆæœ¬:",
            options=["Log å¯¹æ•°å˜æ¢ç‰ˆ (æ¨è)", "åŸå§‹æ•°å€¼ç‰ˆ"],
            index=0, # é»˜è®¤é€‰ Log
            help="å¯¹åº”æ­¥éª¤ 4 ç”Ÿæˆçš„æ–‡ä»¶ã€‚\n- Logç‰ˆ: æ–‡ä»¶åä¸º fusion_final_matrix_log.csvï¼Œé€‚åˆèšç±»åˆ†æã€‚\n- åŸå§‹ç‰ˆ: æ–‡ä»¶åä¸º fusion_final_matrix.csvï¼Œæ•°å€¼æœªå‹ç¼©ã€‚"
        )
    # 2. æ ¹æ®é€‰æ‹©åŠ¨æ€æ„é€ æ–‡ä»¶å
    if "Log" in data_version:
        target_filename = "fusion_final_matrix_log.csv"
    else:
        target_filename = "fusion_final_matrix.csv"
    auto_path = os.path.join(DIRS["result"], target_filename)
    if os.path.exists(auto_path):
        st.success(f"âœ… å·²æ£€æµ‹åˆ°æ–‡ä»¶: {auto_path}")
        df_matrix = pd.read_csv(auto_path, index_col=0)
    else:
        st.warning("âš ï¸ æœªæ‰¾åˆ°è‡ªåŠ¨ç”Ÿæˆçš„æ–‡ä»¶ï¼Œè¯·å…ˆå®Œæˆæ­¥éª¤ 4 æˆ–é€‰æ‹©æ‰‹åŠ¨ä¸Šä¼ ã€‚")
    if df_matrix is not None:
        st.divider()
        st.write(f"ğŸ“Š **å½“å‰æ•°æ®:** {df_matrix.shape[0]} ä¸ªåœ°åŒº, {df_matrix.shape[1]} ä¸ªç‰¹å¾")
        with st.expander("æŸ¥çœ‹æ•°æ®è¯¦æƒ…"):
            st.dataframe(df_matrix.head())
        st.subheader("ğŸ› ï¸ æ¨¡å‹å‚æ•°é…ç½®")
        col1, col2 = st.columns(2)
        with col1:
            n_clusters = st.slider("èšç±»ç±»åˆ«æ•°ç›® (K)", min_value=5, max_value=9, value=6)
        with col2:
            st.markdown("**âš–ï¸ æƒé‡è®¾å®š (ä¸“å®¶æ‰“åˆ†)**")
            weight_settings = {}
            with st.expander("ç‚¹å‡»å±•å¼€è¯¦ç»†æƒé‡è®¾ç½®", expanded=True):
                c1, c2 = st.columns(2)
                with c1:
                    weight_settings["è‡ªç„¶èµ„æºç¦€èµ‹"] = st.number_input("1. è‡ªç„¶èµ„æº", value=5.0, step=0.1)
                with c2:
                    weight_settings["è‡ªç„¶èµ„æº-å¸ƒå°”é¡¹"] = st.number_input("  â†³ æ—åœ°/å¸ƒå°”", value=0.5, step=0.1)
                weight_settings["æ½œåŠ›é¡¹æ•°æ®"] = st.number_input("2. æ½œåŠ›æ•°æ®", value=0.4, step=0.1)
                c3, c4 = st.columns(2)
                with c3: weight_settings["ç©ºé—´å¸ƒå±€"] = st.number_input("3. ç©ºé—´å¸ƒå±€", value=0.1, step=0.05)
                with c4: weight_settings["å­˜åœ¨é—®é¢˜"] = st.number_input("4. å­˜åœ¨é—®é¢˜", value=0.1, step=0.05)
                weight_settings["å­é¡¹ç›®æ•°æ®"] = st.number_input("5. å­é¡¹ç›®", value=0.05, step=0.01)
        c1 = st.columns(1)[0]
        with c1:
            # ç»™æŒ‰é’®ä¸€ä¸ªå”¯ä¸€çš„ key
            start_btn = st.button("ğŸš€ å¼€å§‹èšç±»åˆ†æ", type="primary", key="btn_start_cluster")
        # 3. algorithm
        if start_btn:
            try:
                total_feats = df_matrix.shape[1]
                weights_vec = build_weight_vector(weight_settings, df_matrix.columns)
                print(f"æƒé‡å‘é‡å½¢çŠ¶: {weights_vec.shape}, ç‰¹å¾åˆ—æ•°: {len(df_matrix.columns)}")
                with st.spinner("æ­£åœ¨è¿›è¡Œç†µæƒä¸“å®¶èšç±»..."):
                    df_result, feature_imp, combined_weights, centroids, labels = \
                    clustering_kmeans_with_entropy_expert(
                            df_matrix.values, 
                            df_matrix.index.tolist(), 
                            expert_weights=weights_vec, 
                            n_clusters=n_clusters,
                            path=DIRS["final"]
                        )
                    print('df result',df_result)
                    df_result["Cluster"] = labels + 1
                    # state save
                    st.session_state['cluster_done'] = True
                    st.session_state['cluster_labels'] = labels
                    st.session_state['cluster_centroids'] = centroids
                    st.session_state['cluster_df'] = df_result
                    st.session_state['cluster_weights'] = combined_weights
                    st.success("âœ… èšç±»å®Œæˆï¼")
            except Exception as e:
                st.error(f"èšç±»å¤±è´¥: {e}")
        if st.session_state.get('cluster_done', False):
            df_result = st.session_state['cluster_df']
            centroids = st.session_state['cluster_centroids']
            labels = st.session_state['cluster_labels']
            combined_weights = st.session_state.get('cluster_weights')
            
            st.divider()
            tab_res1, tab_res2, tab_res3 = st.tabs(["ğŸ“‹ ç»“æœæ€»è¡¨", "ğŸ•¸ï¸ ç±»åˆ«ç‰¹å¾åˆ†å¸ƒ(é›·è¾¾å›¾)", "ğŸ“Š åœ°åŒºæ¦‚ç‡åˆ†å¸ƒ(æ¡å½¢å›¾)"])
            with tab_res1:
                st.dataframe(df_result)
                st.download_button("ğŸ“¥ ä¸‹è½½è¯¦ç»†ç»“æœ Excel", 
                                    data=df_result.to_csv().encode('utf-8-sig'),
                                    file_name="clustering_result_full.csv")
            with tab_res2:
                    st.subheader("å„ç±»åˆ«ä¸»è¦å…³æ³¨ç‰¹å¾")
                    # 1. å‡†å¤‡æƒé‡
                    if isinstance(combined_weights, pd.Series):
                        analysis_weights = combined_weights.values
                    else:
                        analysis_weights = combined_weights
                    if analysis_weights.shape[0] != df_matrix.shape[1]:
                         st.error(f"æƒé‡ç»´åº¦ {analysis_weights.shape} ä¸ç‰¹å¾æ•° {df_matrix.shape[1]} ä¸ç¬¦")
                    else:
                        # 2. å‡†å¤‡å®¹å™¨
                        # features = df_matrix.columns.tolist() # ç¡®ä¿æ‹¿åˆ°ç‰¹å¾ååˆ—è¡¨
                        features = df_matrix.columns
                        category_feature_attention = pd.DataFrame(
                            index=features, 
                            columns=[f"Cluster_{i+1}" for i in range(n_clusters)]
                        )
                        # 3. æ ¸å¿ƒè®¡ç®—å¾ªç¯
                        # centroids æ˜¯ (n_clusters, n_features) çš„ numpy æ•°ç»„
                        for k in range(n_clusters):
                            # è·å–ç¬¬ k ç±»çš„ä¸­å¿ƒç‚¹åæ ‡ (å½’ä¸€åŒ–åçš„å¹³å‡å€¼)
                            cluster_center_profile = centroids[k]
                            # === æ ¸å¿ƒå…¬å¼ï¼šä¸­å¿ƒå€¼ Ã— æƒé‡ ===
                           
                            # cluster_profile = cluster_center_profile * analysis_weights
                            cluster_profile = cluster_center_profile
                            # å­˜å…¥ DataFrame
                            category_feature_attention[f"Cluster_{k+1}"] = cluster_profile
                        # 4. è°ƒç”¨ç»˜å›¾
                        try:
                            fig_radar = plot_category_radar_chart(category_feature_attention)
                            st.pyplot(fig_radar)
                            save_radar_path = os.path.join(DIRS["final"], f"{n_clusters}_category_feature_radar.png")
                            fig_radar.savefig(save_radar_path, dpi=300, bbox_inches='tight')
                            
                        except Exception as e_plot:
                            st.error(f"é›·è¾¾å›¾ç»˜åˆ¶å¤±è´¥: {e_plot}")

                        # 5. å±•ç¤ºæ•°æ®è¡¨æ ¼
                        with st.expander("ç‰¹å¾æ³¨æ„åŠ›æ•°å€¼è¯¦æƒ…"):
                            st.markdown("#### ğŸ† å„ç±»åˆ«å…³æ³¨åº¦ Top 10 ç‰¹å¾")
                            st.caption("æ ¼å¼è¯´æ˜ï¼šç‰¹å¾åç§° (æ³¨æ„åŠ›åˆ†å€¼)")
                            top_n = 10
                            rank_data = {}
                            for col in category_feature_attention.columns:
                                # 1. å¯¹æ¯ä¸€åˆ—è¿›è¡Œé™åºæ’åˆ—
                                top_series = category_feature_attention[col].sort_values(ascending=False).head(top_n)
                                
                                # 2. æ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²: "ç‰¹å¾å (0.123)"
                                formatted_vals = [
                                    f"{feat} ({val:.4f})" 
                                    for feat, val in top_series.items()
                                ]
                                # 3. é˜²æ­¢ç‰¹å¾æ€»æ•°å°‘äº10ä¸ªå¯¼è‡´é•¿åº¦ä¸ä¸€è‡´
                                while len(formatted_vals) < top_n:
                                    formatted_vals.append("-")
                                    
                                rank_data[col] = formatted_vals
                            # 4. æ„å»ºæ’å DataFrame
                            df_rank = pd.DataFrame(rank_data)
                            df_rank.index = [f"No.{i+1}" for i in range(top_n)] # è®¾ç½®è¡Œç´¢å¼•ä¸º No.1 ~ No.10
                            # å±•ç¤ºæ’åè¡¨
                            st.dataframe(df_rank, width='content')
                           
            with tab_res3:
                    st.subheader("å„åœ°åŒºå½’å±æ¦‚ç‡å¯è§†åŒ–")
                    
                    with st.expander("ğŸ› ï¸ è§†å›¾è®¾ç½® (åœ°åŒºè¿‡å¤š,ç­›é€‰)", expanded=True):
                        c_view1, c_view2 = st.columns([1, 2])
                        with c_view1:
                            # 1. é€‰æ‹©æ˜¾ç¤ºæ¨¡å¼
                            view_mode = st.radio(
                                "é€‰æ‹©å±•ç¤ºèŒƒå›´:", 
                                ["ğŸ† ä»…å±•ç¤ºå‰ N ä¸ª (é¢„è§ˆ)", "ğŸ” æ‰‹åŠ¨æœç´¢ç‰¹å®šåœ°åŒº", "ğŸ“„ å…¨é‡å±•ç¤º (å¯èƒ½è¾ƒé•¿)"],
                                index=0 # é»˜è®¤åªçœ‹å‰ N ä¸ªï¼Œé˜²æ­¢åˆ·å±
                            )
                        with c_view2:
                            # 2. æ ¹æ®æ¨¡å¼æ˜¾ç¤ºä¸åŒæ§ä»¶
                            df_to_plot = df_result.copy()
                            
                            if "å‰ N ä¸ª" in view_mode:
                                # é»˜è®¤å±•ç¤º 20 ä¸ªï¼Œæœ€å¤§ä¸è¶…è¿‡æ€»æ•°
                                max_val = len(df_result)
                                default_val = min(20, max_val)
                                top_n = st.slider("é€‰æ‹©å±•ç¤ºçš„åœ°åŒºæ•°é‡:", min_value=5, max_value=max_val, value=default_val)
                                
                                # æˆªå–å‰ N è¡Œ (å‡è®¾ df_result å·²ç»æ’å¥½åºï¼Œå¦‚æœæ²¡æœ‰ï¼Œå¯ä»¥å…ˆ sort_index æˆ– sort_values)
                                df_to_plot = df_result.head(top_n)
                                st.caption(f"å½“å‰å±•ç¤º: ç¬¬ 1 è‡³ {top_n} ä¸ªåœ°åŒº")
                                
                            elif "æ‰‹åŠ¨æœç´¢" in view_mode:
                                all_regions = df_result.index.tolist()
                                selected_regions = st.multiselect(
                                    "è¾“å…¥æˆ–é€‰æ‹©è¦æŸ¥çœ‹çš„åœ°åŒº:", 
                                    options=all_regions,
                                    default=all_regions[:5] # é»˜è®¤é€‰å‰5ä¸ªæ¼”ç¤º
                                )
                                if selected_regions:
                                    df_to_plot = df_result.loc[selected_regions]
                                else:
                                    st.warning("âš ï¸ è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªåœ°åŒºã€‚")
                                    df_to_plot = pd.DataFrame() # ç©ºè¡¨
                                    
                            else: # å…¨é‡å±•ç¤º
                                st.info(f"æ­£åœ¨å±•ç¤ºå…¨éƒ¨ {len(df_result)} ä¸ªåœ°åŒºï¼Œå›¾ç‰‡å¯èƒ½è¾ƒé•¿ã€‚")
                                df_to_plot = df_result

                    if not df_to_plot.empty:
                        # 3. è®¡ç®—åŠ¨æ€å›¾è¡¨é«˜åº¦ (ä¼˜åŒ–ä½“éªŒ)
                        # å‡è®¾æ¯ä¸ªæ¡å½¢å  0.4 è‹±å¯¸ï¼ŒåŸºç¡€é«˜åº¦ 2 è‹±å¯¸
                        # è¿™æ ·é€‰ 100 ä¸ªåœ°åŒºæ—¶å›¾ä¼šè‡ªåŠ¨å˜é•¿ï¼Œä¸ä¼šæŒ¤åœ¨ä¸€èµ·
                        dynamic_figsize = (10, max(4, len(df_to_plot) * 0.4))
                        # è°ƒç”¨ä½ çš„ç»˜å›¾å‡½æ•° (æ³¨æ„ï¼šå¦‚æœä½ åŸæ¥çš„å‡½æ•°ä¸èƒ½ä¼  figsizeï¼Œè¿™é‡Œå¯èƒ½éœ€è¦ä¼ æ•´ä¸ª df_to_plot)
                        # å‡è®¾ plot_horizontal_bars_from_df æ¥æ”¶ DataFrame å¹¶è¿”å› figure
                        # å¦‚æœä½ çš„ plot å‡½æ•°å†…éƒ¨å†™æ­»äº† figsizeï¼Œå¯èƒ½éœ€è¦ç¨å¾®æ”¹ä¸€ä¸‹ plot å‡½æ•°è®©å®ƒè‡ªé€‚åº”ï¼Œ
                        # æˆ–è€…ä¾é  matplotlib çš„è‡ªåŠ¨å¸ƒå±€ã€‚
                        try:
                            fig_bars = plot_horizontal_bars_from_df(df_to_plot)
                            
                            # å°è¯•è°ƒæ•´å½“å‰ figure çš„å°ºå¯¸ (å¦‚æœå‡½æ•°å†…éƒ¨æ²¡é”æ­»çš„è¯)
                            fig_bars.set_size_inches(dynamic_figsize)
                            
                            st.pyplot(fig_bars, width='content')
                            # 4. ä¿å­˜
                            # æ³¨æ„ï¼šè¿™é‡Œä¿å­˜çš„æ˜¯"å½“å‰è§†å›¾"çš„å›¾ç‰‡ã€‚
                            # å¦‚æœéœ€è¦ä¿å­˜å…¨é‡å›¾ç‰‡ï¼Œå¯ä»¥åœ¨è¿™é‡Œç”¨ df_result å†ç”»ä¸€æ¬¡ï¼Œæˆ–è€…å‘Šè¯‰ç”¨æˆ·"æ‰€è§å³æ‰€å¾—"
                            save_name = f"{n_clusters}_region_membership_bars_{view_mode}.png"
                            fig_bars_path = os.path.join(DIRS["final"], save_name)
                            fig_bars.savefig(fig_bars_path, dpi=300, bbox_inches='tight')
                            st.caption(f"ğŸ’¾ å½“å‰è§†å›¾å·²ä¿å­˜ä¸º: `{save_name}`")  
                        except Exception as e:
                            st.error(f"ç»˜å›¾å¤±è´¥: {e}")
                    
                    st.success(f"ğŸ‰ æ‰€æœ‰åˆ†æç»“æœï¼ˆè¡¨æ ¼ä¸å›¾è¡¨ï¼‰å·²è‡ªåŠ¨ä¿å­˜è‡³: `{DIRS['final']}`") 
    # === å±•ç¤ºæ–‡ä»¶ç®¡ç† ===
    render_file_manager(DIRS["final"], title="æœ€ç»ˆæˆæœ", file_ext=".png", key_prefix="step5_img")