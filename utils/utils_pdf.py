import fitz  # PyMuPDF
import re
import pikepdf
import io
import os
import fitz  
import difflib


def calculate_global_offset(doc, toc_dict):
    """
    é€šè¿‡å¯¹æ¯”ã€ç›®å½•ä¸­çš„é¡µç ã€‘å’Œã€æ­£æ–‡ä¸­æ ‡é¢˜å®é™…å‡ºç°çš„é¡µç ã€‘ï¼Œè®¡ç®—å…¨å±€åç§»é‡ã€‚
    å·²å¢åŠ ï¼šé˜²ç›®å½•è¯¯åˆ¤é€»è¾‘ï¼ˆé¿å…åŒ¹é…åˆ°ç›®å½•é¡µæœ¬èº«ï¼‰ã€‚
    """
    if not toc_dict: return 0
    
    print("ğŸ”„ æ­£åœ¨åˆ©ç”¨ç›®å½•å†…å®¹è¿›è¡Œåç§»é‡æ ¡å‡† (Anchor Calibration)...")

    # 1. é€‰å–é”šç‚¹
    valid_entries = []
    for title, pages in toc_dict.items():
        start_page = pages[0]
        if start_page >= 1:
            valid_entries.append((title, start_page))
    
    valid_entries.sort(key=lambda x: x[1])
    anchors = valid_entries[:3] # å–å‰3ä¸ª

    if not anchors: return 0

    # 2. éå†é”šç‚¹
    for title, logic_page in anchors:
        # æ‰©å¤§æœç´¢èŒƒå›´ï¼Œä½†è·³è¿‡æå‰éƒ¨ï¼ˆé˜²æ­¢åŒ¹é…åˆ°å°é¢/æ‘˜è¦ï¼‰
        # å‡è®¾ Offset å¯èƒ½å¾ˆå¤§ï¼ˆæ¯”å¦‚å‰è¨€æœ‰15é¡µï¼‰ï¼Œæ‰€ä»¥å¾€åå¤šæœä¸€ç‚¹
        search_start = max(0, logic_page - 5) 
        search_end = min(doc.page_count, logic_page + 30)

        # æ¸…æ´—æ ‡é¢˜
        clean_target = re.sub(r"[^\u4e00-\u9fa5a-zA-Z0-9]", "", title)
        if len(clean_target) < 2: continue

        for i in range(search_start, search_end):
            try:
                page = doc[i]
                
                # === æ ¸å¿ƒä¿®æ”¹ 1: è·å–æ›´è¯¦ç»†çš„æ–‡æœ¬å— ===
                # æˆ‘ä»¬ä¸ä»…è¦çœ‹ textï¼Œè¿˜è¦çœ‹è¿™ä¸€è¡Œé•¿ä»€ä¹ˆæ ·
                # è·å–é¡µé¢ä¸Šéƒ¨ 40%
                header_rect = fitz.Rect(0, 0, page.rect.width, page.rect.height * 0.4)
                
                # è·å– text åŠå…¶å¸ƒå±€ä½ç½®ï¼ŒæŒ‰è¡Œåˆ†å‰²
                page_text = page.get_text("text", clip=header_rect)
                lines = page_text.split('\n')
                
                is_real_header = False
                
                for line in lines:
                    clean_line = re.sub(r"[^\u4e00-\u9fa5a-zA-Z0-9]", "", line)
                    
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«æ ‡é¢˜
                    if clean_target in clean_line:
                        # === æ ¸å¿ƒä¿®æ”¹ 2: æ’é™¤ç›®å½•ç‰¹å¾ ===
                        # ç‰¹å¾ A: åé¢ç´§è·Ÿæ•°å­— (e.g., "ç¬¬ä¸€ç« ... 1")
                        # ç‰¹å¾ B: åŒ…å«å¤§é‡è™šçº¿/ç‚¹ (e.g., "......")
                        
                        # æ£€æŸ¥åŸå§‹ line æ˜¯å¦ä»¥æ•°å­—ç»“å°¾ (å…è®¸å°‘é‡ç©ºæ ¼)
                        if re.search(r'[\.\â€¦\s]+\d+\s*$', line.strip()):
                            # print(f"   [è·³è¿‡] ç¬¬ {i+1} é¡µç–‘ä¼¼ç›®å½•é¡¹: {line.strip()}")
                            continue 
                        # ç‰¹å¾ C: æ£€æŸ¥è¯¥é¡µæ˜¯ä¸æ˜¯æ˜ç¡®å†™ç€â€œç›®å½•â€
                        # (å¦‚æœé¡µé¢æœ€é¡¶ç«¯å†™ç€â€œç›®å½•â€ï¼Œå“ªæ€•è¿™è¡Œæ²¡æœ‰æ•°å­—ä¹Ÿè·³è¿‡)
                        if "ç›®å½•" in page_text[:50] and i < 20: 
                             continue
                        # é€šè¿‡æ‰€æœ‰æ£€æŸ¥ï¼Œè®¤ä¸ºæ˜¯æ­£æ–‡æ ‡é¢˜
                        is_real_header = True
                        break
                if is_real_header:
                    # è®¡ç®—åç§»é‡
                    offset = i - (logic_page - 1)
                    
                    # å†æ¬¡æ ¡éªŒï¼šOffset é€šå¸¸ >= 0
                    if offset >= 0:
                        print(f"âœ… æ ¡å‡†æˆåŠŸï¼é”šç‚¹: '{title}'")
                        print(f"   - ç›®å½•é¡µç : {logic_page}")
                        print(f"   - ç‰©ç†ç´¢å¼•: {i} (ç¬¬ {i+1} é¡µ)")
                        print(f"   - ä¿®æ­£ Offset: {offset}")
                        return offset
            except:
                continue

    print("âš ï¸ æœªèƒ½é€šè¿‡å†…å®¹æ ¡å‡†åç§»é‡ï¼Œé»˜è®¤ Offset = 0")
    return 0


# ========================================================
# è§£æç›®å½•ç”Ÿæˆå­—å…¸
# ========================================================
def parse_toc_to_dict(doc, max_scan_pages=20):
    """
    è§£æPDFç›®å½•ï¼Œè¿”å›ç»“æ„åŒ–å­—å…¸ï¼š
    {
        "clean_title_string": [start_page, end_page],
        ...
    }
    """
    toc_list = [] # ä¸´æ—¶å­˜å‚¨ [(title, page), ...]
    full_toc_text = ""

    # --- A. æå–å‰Né¡µæ–‡æœ¬ ---
    for i in range(min(max_scan_pages, doc.page_count)):
        try:
            page_text = doc[i].get_text()
            if page_text:
                full_toc_text += page_text + "\n"
        except:
            continue

    # --- B. æ¸…æ´—æ–‡æœ¬ (å…³é”®æ­¥éª¤) ---
    # 1. å»é™¤ç›®å½•ä¸­çš„è™šçº¿/ç‚¹ (å¦‚ "......")
    clean_text = re.sub(r"[â€¦\.ï¼]{2,}", " ", full_toc_text)
    # 2. å°è¯•ä¿®å¤æ¢è¡Œ (æœ‰äº›æ ‡é¢˜è¢«æ–­æˆä¸¤è¡Œï¼Œé€šå¸¸ä¸‹ä¸€è¡Œæ˜¯é¡µç )
    # è¿™ä¸€æ­¥æ¯”è¾ƒæ¿€è¿›ï¼Œæ ¹æ®å®é™…æƒ…å†µå¾®è°ƒ
    # clean_text = re.sub(r'\n\s*(\d+)', r' \1', clean_text) 

    # --- C. æ­£åˆ™åŒ¹é… (æå– æ ‡é¢˜ + é¡µç ) ---
    # åŒ¹é…æ¨¡å¼ï¼šè¡Œé¦–(å¯èƒ½å«ç« èŠ‚å·) + å†…å®¹ + ç©ºæ ¼ + é¡µç (è¡Œå°¾)
    # (?m) å¼€å¯å¤šè¡Œæ¨¡å¼
    # ([^\n\d]+?) åŒ¹é…éæ•°å­—çš„æ ‡é¢˜éƒ¨åˆ† (éè´ªå©ª)
    # (\d+) åŒ¹é…é¡µç 
    pattern = r"(?m)^\s*(.*?)\s+(\d+)\s*$"
    matches = re.findall(pattern, clean_text)

    for title, page_str in matches:
        # æ¸…æ´—æ ‡é¢˜ï¼šå»æ‰é¦–å°¾ç©ºæ ¼ã€å»æ‰æœ«å°¾çš„ç‚¹
        clean_title = title.strip().rstrip('.').rstrip()
        # å»æ‰ä¸­é—´çš„æ‰€æœ‰ç©ºç™½å­—ç¬¦ï¼ˆæ–¹ä¾¿åç»­æ¨¡ç³ŠåŒ¹é…ï¼‰
        compact_title = re.sub(r"\s+", "", clean_title)
        
        # è¿‡æ»¤æ‰è¿‡çŸ­çš„è¯¯åˆ¤ (æ¯”å¦‚åªæœ‰ "1")
        if len(compact_title) > 1:
            try:
                page_num = int(page_str)
                # è¿‡æ»¤æ‰é¡µç å¤§å¾—ç¦»è°±çš„è¯¯åˆ¤
                if page_num <= doc.page_count + 10: 
                    toc_list.append((compact_title, page_num))
            except:
                continue

    # --- D. æ„å»ºé—­ç¯å­—å…¸ {Title: [Start, End]} ---
    toc_dict = {}
    total_items = len(toc_list)
    
    if total_items == 0:
        return {}

    for i in range(total_items):
        title, start_p = toc_list[i]
        
        # ç¡®å®šç»“æŸé¡µï¼šé»˜è®¤ä¸ºä¸‹ä¸€æ¡ç›®çš„å¼€å§‹é¡µ
        if i < total_items - 1:
            next_start_p = toc_list[i+1][1]
            # é€»è¾‘ä¿®æ­£ï¼šæœ‰æ—¶å€™ä¸‹ä¸€ç« å¯èƒ½å’Œå½“å‰ç« åœ¨åŒä¸€é¡µï¼Œæˆ–è€…é¡µç å›æº¯ï¼ˆç›®å½•é¡µç é”™è¯¯ï¼‰
            # æˆ‘ä»¬å– max(start_p, next_start_p) ä¿è¯ä¸å€’é€€
            end_p = max(start_p, next_start_p) 
        else:
            # æœ€åä¸€é¡¹ï¼Œç»“æŸé¡µä¸ºæ–‡æ¡£æ€»é¡µæ•°
            end_p = doc.page_count

        toc_dict[title] = [start_p, end_p]
    # print("current file toc_dict:")
    # print(toc_dict)

    return toc_dict
# ========================================================
# åŒ¹é…é€»è¾‘ï¼šåœ¨å­—å…¸ä¸­æŸ¥è¡¨
# ========================================================
def match_section_from_dict(toc_dict, keyword, threshold=0.4, min_pages=1):
    """
    åœ¨ç›®å½•å­—å…¸ä¸­å¯»æ‰¾æœ€åŒ¹é… keyword çš„æ¡ç›® (å¢å¼ºç‰ˆ)
    æ”¹è¿›ç‚¹ï¼š
    1. å¼•å…¥"æ ‡é¢˜çº¯åº¦"ï¼šä¼˜å…ˆåŒ¹é…"å­—æ•°æ›´å°‘ã€æ›´ç²¾å‡†"çš„æ ‡é¢˜ï¼Œè§£å†³çˆ¶å­æ ‡é¢˜åŒ…å«é—®é¢˜ã€‚
    2. å¼•å…¥"é¡µæ•°è¿‡æ»¤"ï¼šè¿‡æ»¤æ‰é¡µæ•°ä¸º0æˆ–è¿‡çŸ­çš„æ— æ•ˆç« èŠ‚ã€‚
    """
    if not toc_dict:
        return None, None, None

    candidates = []
    clean_keyword = re.sub(r"[^\u4e00-\u9fa5a-zA-Z0-9]", "", keyword)
    if not clean_keyword: clean_keyword = keyword

    for title, pages in toc_dict.items():
        start_page, end_page = pages
        page_len = end_page - start_page
        
        # === è¿‡æ»¤æ¡ä»¶ 1: é¡µæ•°æ£€æŸ¥ ===
        if page_len < min_pages:
            continue

        # æ¸…æ´—æ ‡é¢˜ (å»æ‰ "å…­ã€", "(ä¸€)", "1." ç­‰åºå·)
        # è¿™ä¸€æ­¥å¾ˆé‡è¦ï¼Œå¦åˆ™ "(ä¸‰) å­é¡¹ç›®" çš„çº¯åº¦ä¼šæ¯” "å­é¡¹ç›®" ä½
        clean_title_full = re.sub(r"^[ç¬¬\dä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\(\)ï¼ˆï¼‰\.ã€\s]+", "", title)
        # å†æ¬¡æ¸…æ´—ï¼Œå»æ‰ä¸­é—´ç©ºæ ¼
        clean_title = re.sub(r"[^\u4e00-\u9fa5a-zA-Z0-9]", "", clean_title_full)

        # === è¯„åˆ†é€»è¾‘ ===
        # 1. åŸºç¡€ç›¸ä¼¼åº¦ (Fuzzy Match)
        if clean_keyword in clean_title:
            base_score = 1.0
        else:
            base_score = difflib.SequenceMatcher(None, clean_keyword, clean_title).ratio()

        # 2. æ ‡é¢˜çº¯åº¦ (Purity) - è§£å†³çˆ¶å­åŒ…å«é—®é¢˜çš„æ ¸å¿ƒï¼
        # çº¯åº¦ = å…³é”®è¯é•¿åº¦ / æ ‡é¢˜é•¿åº¦
        # ä¾‹å­ï¼š
        # å…³é”®è¯="å­é¡¹ç›®" (3å­—)
        # æ ‡é¢˜A="å»ºè®¾å†…å®¹ä¸å­é¡¹ç›®" (8å­—) -> çº¯åº¦ 0.375
        # æ ‡é¢˜B="å­é¡¹ç›®" (3å­—) -> çº¯åº¦ 1.0
        # ç»“æœï¼šæ ‡é¢˜Bèƒœå‡º
        if len(clean_title) > 0:
            purity_score = len(clean_keyword) / len(clean_title)
            # é˜²æ­¢å…³é”®è¯æ¯”æ ‡é¢˜é•¿å¯¼è‡´çš„ >1
            purity_score = min(1.0, purity_score)
        else:
            purity_score = 0

        # 3. ç»¼åˆå¾—åˆ† (åŠ æƒ)
        # ç›¸ä¼¼åº¦å  60%ï¼Œçº¯åº¦å  40% (çº¯åº¦æƒé‡è¶Šé«˜ï¼Œè¶Šå€¾å‘äºçŸ­æ ‡é¢˜)
        final_score = base_score * 0.6 + purity_score * 0.4
        
        # å¦‚æœåŒ…å«å…³é”®è¯ï¼Œç»™äºˆé¢å¤–å¥–åŠ±ï¼Œç¡®ä¿å®ƒæ¯”å•çº¯çš„æ¨¡ç³ŠåŒ¹é…é«˜
        if clean_keyword in clean_title:
            final_score += 0.2

        if final_score >= threshold:
            candidates.append({
                "title": title,
                "start": start_page,
                "end": end_page,
                "score": final_score,
                "purity": purity_score,
                "len": page_len
            })

    # === æ’åºé€‰ä¼˜ ===
    if candidates:
        # æŒ‰åˆ†æ•°é™åºæ’åˆ—
        candidates.sort(key=lambda x: x["score"], reverse=True)
        
        best = candidates[0]
        print(f"ğŸ” æœç´¢: '{keyword}'")
        print(f"   ğŸ† æœ€ä½³å‘½ä¸­: '{best['title']}' (åˆ†: {best['score']:.2f}, çº¯åº¦: {best['purity']:.2f}, é¡µæ•°: {best['len']})")
        
        # æ‰“å°å…¶ä»–å€™é€‰ï¼ˆè°ƒè¯•ç”¨ï¼‰
        if len(candidates) > 1:
            second = candidates[1]
            print(f"   ğŸ¥ˆ æ¬¡é€‰åŒ¹é…: '{second['title']}' (åˆ†: {second['score']:.2f})")

        return best["start"], best["end"], best["title"]

    return None, None, None

# ========================================================
# è£å‰ªå‡½æ•°
# ========================================================
def extract_section_to_pdf(pdf_path, output_path, section_keyword="é—®é¢˜"):
    src_doc = None
    out_doc = None
    try:
        src_doc = fitz.open(pdf_path)
       
        # 2. è§£æç›®å½•
        print("æ­£åœ¨è§£æç›®å½•ç»“æ„...")
        toc_dict = parse_toc_to_dict(src_doc)
        
        offset = calculate_global_offset(src_doc, toc_dict)
        print(f"ğŸ“„ æ–‡æ¡£æ€»é¡µæ•°: {src_doc.page_count}, è®¡ç®—åç§»é‡ Offset = {offset}")
        
        if not toc_dict:
            print("âš ï¸ æ–‡æœ¬ç›®å½•è§£æå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ä¹¦ç­¾...")
            return False

        # 3. åŒ¹é…åŒºé—´ (å¾—åˆ°çš„æ˜¯ç›®å½•ä¸Šçš„é€»è¾‘é¡µç ï¼Œä¾‹å¦‚ 5 -> 8)
        start_logic, end_logic, matched_title = match_section_from_dict(toc_dict, section_keyword)
        
        if start_logic is None:
            print("âŒ æœªæ‰¾åˆ°åŒ¹é…ç« èŠ‚")
            return False
        
        start_idx = start_logic + offset - 1
        
        # å¤„ç†æœ€åä¸€ç« çš„æƒ…å†µ (end_logic ä¸º 99999)
        if end_logic > 90000:
            end_idx = src_doc.page_count - 1 # ç›´åˆ°æ–‡æ¡£æœ«å°¾
        else:
            end_idx = end_logic + offset - 1
        # 5. è¾¹ç•Œä¿®æ­£
        if start_idx < 0: start_idx = 0
        if start_idx >= src_doc.page_count: 
            print("âŒ è®¡ç®—å‡ºçš„èµ·å§‹é¡µè¶…å‡ºæ–‡æ¡£èŒƒå›´")
            return False
            
        if end_idx >= src_doc.page_count: end_idx = src_doc.page_count - 1
        
        # å…³é”®ä¿®æ­£ï¼šå¦‚æœç®—å‡ºæ¥çš„ end_idx æ¯” start_idx è¿˜å°ï¼ˆç›®å½•é¡µç æ ‡é”™äº†ï¼‰ï¼Œå¼ºåˆ¶å–ä¸€é¡µ
        if end_idx < start_idx: 
            # å°è¯•å¾€åå¤šå–å‡ é¡µï¼Œæ¯”å¦‚é»˜è®¤æå– 3 é¡µ
            print("âš ï¸ ç»“æŸé¡µç å¼‚å¸¸ï¼Œé»˜è®¤æå– 3 é¡µ")
            end_idx = min(start_idx + 2, src_doc.page_count - 1)

        print(f"âœ… æ‰§è¡Œè£å‰ª: {matched_title}")
        print(f"   é€»è¾‘é¡µç : {start_logic} -> {end_logic}")
        print(f"   ç‰©ç†ç´¢å¼•: {start_idx} -> {end_idx} (Offset={offset})")

        out_doc = fitz.open()
        # insert_pdf çš„ to_page æ˜¯åŒ…å«åœ¨å†…çš„ï¼Œæ‰€ä»¥ä¸éœ€è¦ -1
        out_doc.insert_pdf(src_doc, from_page=start_idx, to_page=end_idx)
        out_doc.save(output_path)
        return True

    except Exception as e:
        print(f"è£å‰ªè¿‡ç¨‹å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        return False
    finally:
        if src_doc: src_doc.close()
        if out_doc: out_doc.close()



def open_pdf_auto_repair(pdf_path):
    """
    å°è¯•æ‰“å¼€ PDF çš„é€šç”¨å·¥å…·å‡½æ•°ã€‚
    ä¼˜å…ˆå°è¯• fitz ç›´æ¥æ‰“å¼€ï¼Œå¤±è´¥åˆ™è°ƒç”¨ pikepdf ä¿®å¤æµã€‚
    """
    try:
        return fitz.open(pdf_path)
    except Exception as e:
        # print(f"fitz æ‰“å¼€å¤±è´¥: {e}ï¼Œå°è¯•ä¿®å¤...")
        try:
            with pikepdf.open(pdf_path, allow_overwriting_input=True) as p:
                mem_stream = io.BytesIO()
                p.save(mem_stream)
                mem_stream.seek(0)
                return fitz.open("pdf", mem_stream)
        except Exception:
            return None

'''
def compute_page_offset(pdf_path, max_pages_to_check=20):
    """è®¡ç®—é¡µç åç§»é‡"""
    doc = None
    try:
        doc = open_pdf_auto_repair(pdf_path)
        if doc is None: return 0

        for i in range(min(max_pages_to_check, doc.page_count)):
            try:
                page = doc[i]
                text = page.get_text()
                if not text: continue
                lines = [line.strip() for line in text.split("\n") if line.strip()]
                if not lines: continue
                
                last_line = lines[-1]
                match = re.search(r"(?:ç¬¬\s*(\d+)\s*é¡µ)|^\s*(\d+)\s*$", last_line)
                if match:
                    logical_page = int(match.group(1)) if match.group(1) else int(match.group(2))
                    offset = i - (logical_page - 1)
                    return offset
            except:
                continue
    except:
        return 0
    finally:
        if doc: doc.close()
    return 0
'''

'''
def find_section_pages(pdf_path, section_title="é—®é¢˜"):
    """æŸ¥æ‰¾ç« èŠ‚èµ·æ­¢é¡µç """
    start_page, end_page = None, None
    doc = None

    try:
        doc = open_pdf_auto_repair(pdf_path)
        if doc is None: return None, None

        toc_text = ""
        # æ‰«æå‰ 20 é¡µä½œä¸ºç›®å½•
        for i in range(min(20, doc.page_count)):
            try:
                page_text = doc[i].get_text()
                if page_text: toc_text += page_text + "\n"
            except: continue
        clean_toc_text = re.sub(r"[â€¦\.ï¼]{2,}", " ", toc_text)
        clean_toc_text = re.sub(r'(?m)^\s*([ï¼ˆ(]?\s*[\dä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å][\d.ï¼)ï¼‰]*[ã€]?)\s*\n', r'\1 ', clean_toc_text)
        pattern = r"(?m)^\s*([ï¼ˆ(]?\s*[\dä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å].*?)\s+(\d+)\s*$"
        matches = re.findall(pattern, clean_toc_text)

        toc = []
        for title, page in matches:
            clean_title = title.strip().rstrip('.').rstrip()
            compact_title = re.sub(r"\s+", "", clean_title)
            toc.append((compact_title, int(page)))
            
        for idx, (title, page) in enumerate(toc):
            if section_title in title:
                start_page = page
                found_valid_end = False
                for next_idx in range(idx + 1, len(toc)):
                    candidate_end = toc[next_idx][1]
                    if candidate_end > start_page:
                        end_page = candidate_end
                        found_valid_end = True
                        break 
                if not found_valid_end:
                    end_page = doc.page_count + 1
                break 
                
    except:
        return None, None
    finally:
        if doc: doc.close()

    return start_page, end_page
'''

'''
def extract_section_to_pdf(pdf_path, output_path, section_title="é—®é¢˜"):
    """æ‰§è¡Œè£å‰ªä¸»é€»è¾‘"""
    src_doc = None
    out_doc = None
    try:
        offset = compute_page_offset(pdf_path)
        start_logic, end_logic = find_section_pages(pdf_path, section_title)
        
        if start_logic is None or end_logic is None:
            return False

        src_doc = open_pdf_auto_repair(pdf_path)
        if not src_doc: return False

        start_idx = start_logic + offset - 1
        end_idx = end_logic + offset - 1
        
        # è¾¹ç•Œä¿®æ­£
        if start_idx < 0: start_idx = 0
        if end_idx > src_doc.page_count: end_idx = src_doc.page_count
        if start_idx >= end_idx: return False

        out_doc = fitz.open()
        out_doc.insert_pdf(src_doc, from_page=start_idx, to_page=end_idx - 1)
        out_doc.save(output_path)
        return True
    except:
        return False
    finally:
        if src_doc: src_doc.close()
        if out_doc: out_doc.close()     
'''

def extract_section_to_pdf_self(pdf_path, start, end, output_path):
    """
    æŒ‰æŒ‡å®šé¡µç è£å‰ª PDF å¹¶ä¿å­˜ (PyMuPDF å¢å¼ºç‰ˆ)
    start/end: é€»è¾‘é¡µç  (ä» 1 å¼€å§‹)
    end: ç»“æŸé¡µç  (ä¸åŒ…å«ï¼Œä¸ Python range ä¹ æƒ¯ä¸€è‡´ï¼Œä¾‹å¦‚ start=1, end=3 æå–ç¬¬1,2é¡µ)
         (æ³¨æ„ï¼šè¯·ç¡®è®¤æ‚¨çš„è°ƒç”¨é€»è¾‘ï¼Œå¦‚æœ end æ˜¯åŒ…å«çš„ï¼Œè¯·åœ¨ä¸‹æ–¹ indices è®¡ç®—æ—¶è°ƒæ•´)
    """
    offset = 0  # é»˜è®¤ä¸åç§»ï¼Œå¦‚æœéœ€è¦è‡ªåŠ¨è®¡ç®—åç§»ï¼Œå¯è°ƒç”¨ compute_page_offset
    src_doc = None
    out_doc = None
    
    try:
        # 1. ä½¿ç”¨è‡ªåŠ¨ä¿®å¤åŠŸèƒ½æ‰“å¼€æºæ–‡ä»¶
        src_doc = open_pdf_auto_repair(pdf_path)
        if not src_doc:
            print(f"âŒ æ— æ³•æ‰“å¼€æˆ–ä¿®å¤æ–‡ä»¶: {pdf_path}")
            return False

        # 2. è½¬æ¢é¡µç ä¸ºç‰©ç†ç´¢å¼• (0-based)
        # ç”¨æˆ·ä¼ å…¥çš„ start æ˜¯ 1-basedï¼Œæ‰€ä»¥å‡ 1
        start_idx = start + offset - 1
        # ç”¨æˆ·ä¼ å…¥çš„ end æ˜¯ 1-based ä¸”é€šå¸¸ä½œä¸º range çš„ç»“å°¾ (exclusive)ï¼Œæ‰€ä»¥å‡ 1
        end_idx = end + offset - 1
        
        # è¾¹ç•Œæ£€æŸ¥
        if start_idx < 0: start_idx = 0
        if end_idx > src_doc.page_count: end_idx = src_doc.page_count
        
        if start_idx >= end_idx:
            print(f"âš  é¡µç èŒƒå›´æ— æ•ˆæˆ–ä¸ºç©º: {start}-{end} (Indices: {start_idx}-{end_idx})")
            return False

        # 3. æå–å¹¶ä¿å­˜
        out_doc = fitz.open()
        
        # fitz.insert_pdf çš„å‚æ•° from_page æ˜¯åŒ…å«çš„ï¼Œto_page ä¹Ÿæ˜¯åŒ…å«çš„
        # æˆ‘ä»¬è¦æå– [start_idx, end_idx) åŒºé—´
        # æ‰€ä»¥ to_page åº”è¯¥æ˜¯ end_idx - 1
        out_doc.insert_pdf(src_doc, from_page=start_idx, to_page=end_idx - 1)
        
        out_doc.save(output_path)
        print(f"è‡ªå®šä¹‰å¤„ç†å®Œæˆ -> {os.path.basename(output_path)}")
        return True

    except Exception as e:
        print(f"âŒ è‡ªå®šä¹‰æå–å¤±è´¥: {e}")
        return False
    finally:
        # ç¡®ä¿å…³é—­æ–‡ä»¶å¥æŸ„
        if src_doc: src_doc.close()
        if out_doc: out_doc.close()

def parser_file(filename):
    """
    è§£ææ–‡ä»¶åï¼Œè¿”å›å­—å…¸ã€‚
    """
    city = "æœªçŸ¥åŸå¸‚"
    district = "-"
    unit = ""
    
    # 1. å¼ºåŠ›æ¸…æ´—ï¼šå»æ‰ .pdf å’Œæ‰€æœ‰å¯èƒ½çš„ä»»åŠ¡åç¼€
    clean_name = filename.replace(".pdf", "")
    
    if '_' in clean_name:
        clean_name = clean_name.split('_')[0]
    
    # === ç­–ç•¥ A: å¤„ç†çŸ­æ¨ªçº¿æ ¼å¼ (City-District) ===
    if '-' in clean_name:
        parts = clean_name.split('-')
        if len(parts) >= 1: city = parts[0]
        if len(parts) >= 2: district = parts[1]
        if len(parts) >= 3: unit = parts[2]
            
        return {
            "åŸå§‹æ–‡ä»¶å": filename,
            "æ–‡ä»¶å": clean_name,  # ç›´æ¥ä½¿ç”¨æ¸…æ´—åçš„åå­—
            "åŸå¸‚": city,
            "åœ°åŒº/å¿": district,
            "è¯¦ç»†å•å…ƒ": unit if unit else "æ— "
        }
        
def extract_info(filename):
    """
    è§£ææ–‡ä»¶åï¼Œè¿”å›å­—å…¸ã€‚
    å…¼å®¹ä¸¤ç§æ¨¡å¼ï¼š
    1. å·²æ¸…æ´—è¿‡çš„æ ¼å¼ï¼š'ä¸œè-å‡¤å²—_landuse.pdf' -> æå–ä¸º ä¸œè, å‡¤å²—
    2. åŸå§‹é•¿æ–‡ä»¶åï¼š'ä¸œèå¸‚å‡¤å²—é•‡å…¨åŸŸ...pdf' -> æå–ä¸º ä¸œèå¸‚, å‡¤å²—é•‡
    """
    city = "æœªçŸ¥åŸå¸‚"
    district = "-" 
    unit = ""
    
    # 1. åŸºç¡€æ¸…æ´—ï¼šå»æ‰ .pdf
    clean_name = filename.replace(".pdf", "")
    
    # 2. å‰¥ç¦»ä»»åŠ¡åç¼€ (è¿™æ˜¯å…³é”®ï¼æŠŠ _landuse, _issue ç­‰å»æ‰ï¼Œè¿˜åŸæˆ åœ°åŒº-åŒºå¿)
    # æ­£åˆ™è§£é‡Šï¼šåŒ¹é…ä¸‹åˆ’çº¿å¼€å¤´ï¼Œåé¢è·Ÿç€ä»»åŠ¡åï¼Œç›´åˆ°å­—ç¬¦ä¸²ç»“æŸ
    clean_name = re.sub(r'(_landuse|_issue|_potential|_project|_spatial|_data|_cropped|_manual).*$', '', 
                        clean_name,
                        flags=re.IGNORECASE)

    # === ç­–ç•¥ A: å¤„ç†å·²ç»æ¸…æ´—è¿‡çš„çŸ­æ¨ªçº¿æ ¼å¼ (City-District-Unit) ===
    # å¦‚æœåå­—é‡Œæœ‰æ¨ªçº¿ï¼Œè¯´æ˜è¿™æ˜¯æˆ‘ä»¬è‡ªå·±ç”Ÿæˆçš„æ–‡ä»¶ï¼Œç›´æ¥åˆ‡åˆ†å³å¯
    if '-' in clean_name:
        parts = clean_name.split('-')
        # åªæœ‰ä¸€æ®µï¼š 'ä¸œè'
        if len(parts) >= 1:
            city = parts[0]
        # æœ‰ä¸¤æ®µï¼š 'ä¸œè-å‡¤å²—'
        if len(parts) >= 2:
            district = parts[1]
        # æœ‰ä¸‰æ®µï¼š 'ä¸œè-å‡¤å²—-å®˜äº•å¤´'
        if len(parts) >= 3:
            unit = parts[2]
            
        return {
            "åŸå§‹æ–‡ä»¶å": filename,
            "æ–‡ä»¶å": clean_name, # ç”¨äºæ˜¾ç¤ºçš„çº¯åœ°åŒºå
            "åŸå¸‚": city,
            "åœ°åŒº/å¿": district if district else "-",
            "è¯¦ç»†å•å…ƒ": unit if unit else "æ— "
        }

    # === ç­–ç•¥ B: å¤„ç†åŸå§‹é•¿æ–‡ä»¶å (Regex åŒ¹é…) ===
    # åŒ¹é…è§„åˆ™ï¼šä»¥"å¸‚"ç»“å°¾çš„å‰ç¼€ + ä¸­é—´åŒºåŸŸå + å…³é”®è¯
    match = re.search(r'^(.+?å¸‚)(.+?)(?:å…¨åŸŸ|å®æ–½|é¡¹ç›®|æ°¸ä¹…|åœŸåœ°)', clean_name)
    if match:
        city = match.group(1)
        district = match.group(2)
    else:
        # ç‰¹æ®Šè§„åˆ™å…œåº•
        if "å¹¿å·å¸‚-æ¹›æ±Ÿå¸‚" in filename:
            city = "å¹¿å·æ¹›æ±Ÿåˆä½œå›­"
            district = "å¥‹å‹‡é«˜æ–°åŒº"
        elif "å¸‚" in clean_name and city == "æœªçŸ¥åŸå¸‚":
            # æœ€åçš„å°è¯•ï¼šæŒ‰â€œå¸‚â€å­—åˆ‡åˆ†
            try:
                idx = clean_name.index("å¸‚")
                city = clean_name[:idx+1]
                district = clean_name[idx+1:]
            except: pass
    # æå–æ‹¬å·å†…å®¹
    unit_match = re.search(r'[ï¼ˆ\(](.+?)[ï¼‰\)]', filename)
    if unit_match:
        unit = unit_match.group(1)
    
    short_city = city.replace("å¸‚", "")
    short_district = district
    
    # æ¸…æ´—åŒºå¿åç¼€
    for suffix in ["å¸‚", "åŒº", "å¿", "é•‡", "è¡—é“", "è‡ªæ²»å¿", "æ–°åŒº", "ç®¡ç†åŒº", "å¼€å‘åŒº", "ç‰¹åˆ«åˆä½œåŒº"]:
        if short_district.endswith(suffix) and len(short_district) > len(suffix):
            if short_district == "å—åŒº" and suffix == "åŒº": continue
            short_district = short_district.replace(suffix, "")
            break
            
    short_unit = unit
    for suffix in ["å®æ–½å•å…ƒ", "å•å…ƒ", "é•‡", "è¡—é“", "ç‰‡åŒº", "å®æ–½æ–¹æ¡ˆ"]:
        short_unit = short_unit.replace(suffix, "")
    
    # ç»„è£…æ ‡å‡†åŒ–åå­—
    components = [short_city]
    if short_district and short_district != "-": components.append(short_district)
    if short_unit: components.append(short_unit)
    
    new_name = "-".join(components)

    return {
        "åŸå§‹æ–‡ä»¶å": filename,
        "æ–‡ä»¶å": new_name,
        "åŸå¸‚": city,
        "åœ°åŒº/å¿": short_district if short_district else "-",
        "è¯¦ç»†å•å…ƒ": unit if unit else "æ— "
    }
    
    
def extract_pages_by_keywords(pdf_path, output_path, keyword_pattern_str):
    """
    æ‰«ææ¯ä¸€é¡µå†…å®¹ï¼ŒåŒ¹é…å…³é”®è¯ï¼ˆæ”¯æŒæ­£åˆ™è¡¨è¾¾å¼ï¼‰ã€‚
    å¦‚æœæ‰¾åˆ°æ ‡é¢˜ï¼Œä¸”åç»­é¡µé¢æ˜¯è¿ç»­è¡¨æ ¼ï¼Œä¼šè‡ªåŠ¨åˆå¹¶åç»­é¡µé¢ã€‚
    """
    pages_to_save = []
    in_table = False
    
    try:
        search_pattern = re.compile(keyword_pattern_str)
    except:
        # å¦‚æœç”¨æˆ·è¾“å…¥çš„ä¸æ˜¯æ­£åˆ™ï¼Œè½¬ä¸ºæ™®é€šåŒ…å«åŒ¹é…
        search_pattern = re.compile(re.escape(keyword_pattern_str))

    src_doc = None
    out_doc = None
    
    try:
        src_doc = open_pdf_auto_repair(pdf_path)
        if not src_doc: return False
        
        for page_index, page in enumerate(src_doc):
            text = page.get_text() or ""
            
            # åˆ¤æ–­æ˜¯å¦åŒ…å«æ ‡é¢˜
            has_title = bool(search_pattern.search(text))
            
            # åˆ¤æ–­æ˜¯å¦æœ‰è¡¨æ ¼ (PyMuPDF åŠŸèƒ½)
            # find_tables æ¯”è¾ƒè€—æ—¶ï¼Œä»…åœ¨å¿…è¦æ—¶è°ƒç”¨æˆ–æ¯ä¸€é¡µè°ƒç”¨
            tables = page.find_tables()
            has_table = len(tables.tables) > 0
            
            if has_title:
                in_table = True
                pages_to_save.append(page_index)
            elif in_table and has_table:
                # å¦‚æœå¤„äº"è¡¨æ ¼è¿ç»­æ¨¡å¼"ä¸”å½“å‰é¡µä¹Ÿæœ‰è¡¨æ ¼ï¼Œåˆ¤å®šä¸ºè·¨é¡µè¡¨æ ¼
                pages_to_save.append(page_index)
            else:
                # æ–­å¼€è¿ç»­
                in_table = False
        
        if not pages_to_save:
            return False
            
        # ä¿å­˜ç»“æœ
        out_doc = fitz.open()
        for p_idx in pages_to_save:
            out_doc.insert_pdf(src_doc, from_page=p_idx, to_page=p_idx)
            
        out_doc.save(output_path)
        return True

    except Exception as e:
        print(f"å…³é”®è¯æå–å¤±è´¥: {e}")
        return False
    finally:
        if src_doc: src_doc.close()
        if out_doc: out_doc.close()


        
def dict_save2csv(data: dict, save_path: str):
    """
    å°†å­—å…¸æ•°æ®ä¿å­˜ä¸º CSV æ–‡ä»¶
    """
    import pandas as pd
    df = pd.DataFrame.from_dict(data, orient='index')
    df.reset_index(inplace=True)
    df.rename(columns={'index': 'åœ°åŒº'}, inplace=True)
    df.to_csv(save_path, index=False, encoding='utf-8-sig')
    print(f"æ•°æ®å·²ä¿å­˜åˆ° {save_path}")
